{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "---\n",
    "description: Examples to show how to use litesearch to build a coding assistant with RAG\n",
    "  capabilities\n",
    "title: tool-use\n",
    "\n",
    "---"
   ],
   "id": "b3c814e167123f58"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "#| default_exp code-assistant",
   "id": "bcf6aae51dd40592",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Setup\n",
    "> Let's set up our coding assistant with RAG capabilities using litesearch for code search and embedding models for semantic search.\n"
   ],
   "id": "c5af511907d5579a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "b090fb17d6ad3a39"
  },
  {
   "cell_type": "code",
   "id": "9ea9b9b51027a658",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "import ast\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-12-04T01:34:52.076556Z",
     "start_time": "2025-12-04T01:34:50.378708Z"
    }
   },
   "source": [
    "#| export\n",
    "from ddgs import DDGS\n",
    "from fastcore.all import *\n",
    "from fastlite import Database\n",
    "import json\n",
    "from lisette import *\n",
    "from litesearch import *\n",
    "import os\n",
    "from toolslm.funccall import *"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying usearch macOS fix if required...\n",
      "usearch dylib path:  /Users/71293/code/litesearch/.venv/bin/usearch_binaries/usearch_sqlite.dylib\n",
      "✗ install_name_tool failed: Command '['install_name_tool', '-add_rpath', '/usr/lib', '/Users/71293/code/litesearch/.venv/bin/usearch_binaries/usearch_sqlite.dylib']' returned non-zero exit status 1.\n",
      "Command output: \n",
      "Command stderr: error: /Library/Developer/CommandLineTools/usr/bin/install_name_tool: for: /Users/71293/code/litesearch/.venv/bin/usearch_binaries/usearch_sqlite.dylib (for architecture arm64) option \"-add_rpath /usr/lib\" would duplicate path, file already has LC_RPATH for: /usr/lib\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "6ac0a500fa7ecd96",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## CodeSearcher\n",
    "Let's set up the codesearcher db which will handle code ingestion, embedding and searching.\n",
    ">We'll use modern-bert-base onnx model for embedding the code snippets. The `FastEncode` class from utils will help us with that."
   ]
  },
  {
   "cell_type": "code",
   "id": "1f70c9bd3cc45e47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T01:45:01.656613Z",
     "start_time": "2025-12-04T01:45:01.614323Z"
    }
   },
   "source": [
    "#| export\n",
    "os.environ['TOKENIZERS_PARALLELISM']='false'  # to suppress warnings from tokenizers\n",
    "def codesearcher(db_pth='code.db'):\n",
    "\t'A simple code search database using litesearch and onnx embeddings.'\n",
    "\tdb = database(db_pth)\n",
    "\tst = db.get_store(hash=True)\n",
    "\tif 'package' not in st.c: st.add_column('package', str)\n",
    "\tdb.t.packages.create(name=str, version=str, summary=str, uploaded_at=float, pk=['name'],\n",
    "\t\tdefaults=dict(uploaded_at='CURRENT_TIMESTAMP'), not_null=['name'], if_not_exists=True)\n",
    "\treturn db\n",
    "\n",
    "@patch(as_prop=True)\n",
    "def st(self:Database): return self.get_store(hash=True)\n",
    "@patch(as_prop=True)\n",
    "def packages(self:Database): return self.t.packages\n",
    "\n",
    "@timed_cache(3600*24)\n",
    "def embedder(emb_model=modernbert): return FastEncode(emb_model)\n",
    "emb_doc = lambda t: embedder().encode_document(t)\n",
    "emb_query = lambda t: embedder().encode_query(t)\n",
    "\n",
    "@patch\n",
    "def _is_pkg_ingested(self:Database, pkg:str) -> bool:\n",
    "\t'Check if a package is already ingested and up-to-date.'\n",
    "\tfrom importlib.metadata import version as v\n",
    "\tep = self.packages(select='name, version', where=f'name={pkg!r}')\n",
    "\treturn (only(ep)['version'] == v(pkg)) if ep else False\n",
    "\n",
    "@patch\n",
    "def update_pkg(self:Database, pkg:str, embed=True):\n",
    "\t'Update package metadata in the packages table.'\n",
    "\tif self._is_pkg_ingested(pkg) and len(self.st(where=f'package={pkg!r}')) > 0: return\n",
    "\tfrom importlib.metadata import version as v, metadata as meta\n",
    "\tcontent = pkg2chunks(pkg).map(lambda d: d.update(dict(package=pkg,uploaded_at=d['metadata']['uploaded_at'],metadata=d['metadata'])) or d)\n",
    "\tif ex := self.st(select='content', where=f'package={pkg!r}'):\n",
    "\t\tdef slug(s): from hashlib import md5; return md5(s.encode('utf-8')).hexdigest()\n",
    "\t\thash_ = lambda r: slug(r['content'])\n",
    "\t\tif del_ids := set(ex.map(hash_)).difference(content.map(hash_)): self.st.delete(del_ids)\n",
    "\tif not content: return\n",
    "\tif embed: content = L(chunked(content, 100)).map(embed_chunk).concat()\n",
    "\tself.st.insert_all(content,upsert=True,hash_id='id')\n",
    "\tself.packages.upsert(dict(name=pkg, version=v(pkg)), summary=meta(pkg)['Summary'], pk='name')\n",
    "\n",
    "@patch\n",
    "def rm_pkg(self:Database, pkg:str):\n",
    "\t'Remove a package and its code snippets from the database.'\n",
    "\tself.st.delete(where=f'package={pkg!r}')\n",
    "\tself.packages.delete(where=f'name={pkg!r}')\n",
    "\n",
    "def embed_chunk(chunk:list,emb_fn=emb_doc):\n",
    "\t'Embed a list of code chunks using emb_f'\n",
    "\tif not (chunk and embedder()): return\n",
    "\tc = [b['content'] for b in chunk if b['content'].strip()]\n",
    "\tif not c: return\n",
    "\tfor e,b in zip(emb_fn(c),chunk): b['embedding'] = e.tobytes()\n",
    "\treturn chunk\n",
    "\n",
    "@patch\n",
    "def embed(self:Database, sz=100, reembed=False):\n",
    "\t'Embed all documents in a table using emb_f'\n",
    "\tif not (self.st and embedder()): return\n",
    "\tb=L(chunked(self.st(where=f'embedding is NULL' if not reembed else None), sz)).map(embed_chunk).concat()\n",
    "\tself.st.upsert_all(b, pk='id')\n",
    "\n",
    "@patch\n",
    "def update_pkgs(self:Database, pkgs:str|list, embed=True):\n",
    "\tpkgs = set(pkgs).union(L(self.packages(select='name')).map('name'))\n",
    "\tif not pkgs: return\n",
    "\tL(pkgs).map(lambda p:self.update_pkg(p,embed))\n",
    "\n",
    "@patch\n",
    "def code_search(self:Database,\n",
    "            q:str,               \t# query to search\n",
    "\t\t\temb_q:str=None,     \t# query to embed. If None, use q\n",
    "\t\t\twide:bool=False,    \t# whether to use wide search\n",
    "\t\t\temb_fn=emb_query,\t\t# embedding function\n",
    "\t\t\t**kw\t\t\t\t\t# additional args to pass to db.search\n",
    "\t):\n",
    "\t\t'Code search through the database to find relevant code snippets.'\n",
    "\t\temb = emb_fn(emb_q if emb_q else q)\n",
    "\t\tavailable = L(set(q.split(' ')).intersection(L(self.packages(select='name')).map('name')))\n",
    "\t\twh = f'package in ({','.join(available.map(repr))})' if available else None\n",
    "\t\tkw['where'] = wh if 'where' not in kw else f\"({kw['where']}) AND {wh}\" if wh else kw['where']\n",
    "\t\treturn self.search(pre(q, wide=wide), emb.tobytes(), **kw)"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "You can now create a codesearcher database and ingest code from installed packages. I'm giving it a few popular packages to start with.",
   "id": "ce956ed9e5445ce3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T01:38:38.273691Z",
     "start_time": "2025-12-04T01:38:38.208061Z"
    }
   },
   "cell_type": "code",
   "source": [
    "db=codesearcher()\n",
    "p = installed_packages(['fastcore','fastlite','lisette','toolslm','chonkie','model2vec','ghapi','apswutils','apsw','fasthtml','litesearch'])\n",
    "db.update_pkgs(p, embed=True)"
   ],
   "id": "4140ae0233c94ba4",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Coding Assistant",
   "id": "b25ab5c0337b8a96"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Coding Assistant Tools\n",
    "> Let's set up the tools that our coding assistant will use. We'll have a web search tool, a code search tool using RAG, and a code execution tool."
   ],
   "id": "1408c886b4dfebdd"
  },
  {
   "cell_type": "code",
   "id": "372aec612a248be6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T01:39:37.728145Z",
     "start_time": "2025-12-04T01:39:37.723179Z"
    }
   },
   "source": [
    "#| export\n",
    "def websearch(q: str,               # query to search\n",
    "              top_k: int = 10,      # number of top results to return\n",
    "              ):\n",
    "\t'Web search results reranked with flashrank'\n",
    "\tres = dict2obj(DDGS().text(q, max_results=top_k))\n",
    "\treturn json.dumps([dict(text=r.body, id=r.href, meta=dict(title=r.title)) for r in res])\n",
    "\n",
    "def rag(q: str,\t# query string\n",
    "        emb_q: str = None, # query to embed. If None, use q\n",
    "        top_k: int = 50, # number of top results to return\n",
    "        wide: bool = False, # whether to use wide search\n",
    "        web: bool = False # whether to include web search results\n",
    ") -> str:\n",
    "    'Search indexed code for relevant chunks. Returns structured results.'\n",
    "    r = db.code_search(q, emb_q=emb_q if emb_q else q, limit=top_k, columns=['content', 'metadata'], wide=wide, emb_metric='cosine')\n",
    "    if web: r += websearch(q, top_k=top_k // 2)  # Balance\n",
    "    return json.dumps(dict(query=q, results=r, top_k=top_k))  # Or dict if no Pydantic\n",
    "\n",
    "def run_code(code:str, \t  # code to run\n",
    "             strict=True # whether to run in strict mode\n",
    "             )->str:\n",
    "\t'''Run code in python interpreter'''\n",
    "\tif not strict: (code, globals())\n",
    "\treturn python(code)\n",
    "\n",
    "def get_globals():\n",
    "\t'Return current global variables.'\n",
    "\treturn globals().keys()"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### LLM Chat with Tools Setup.\n",
    "> Let's not call it an Agent. Maybe an Augmented LLM Chat?"
   ],
   "id": "235f8986d76deae9"
  },
  {
   "cell_type": "code",
   "id": "c0737f785e25504",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T06:10:07.204607Z",
     "start_time": "2025-12-04T06:10:07.195387Z"
    }
   },
   "source": [
    "#| export\n",
    "sp= f'''You are a razor-sharp Python coding assistant with perfect knowledge of fastcore, fastlite, lisette, toolslm, chonkie, model2vec and every package indexed in your RAG database.\n",
    "\n",
    "Your ONLY job: answer with concise, working, copy-pasteable code. No essays. No apologies.\n",
    "Available packages: {db.packages(select='name,summary')}.\n",
    "TOOLS (in exact order you MUST follow):\n",
    "1. rag(q: str, emb_q: str | None, top_k: int = 10, wide:bool=False, web:bool=False) → ALWAYS call this first on every code question.\n",
    "   - Use natural language q for FTS5, Do not pass the user query as is to q. Clean the query first. Add package names if the query matches any ingested package. Use the summary of packages to help you to add the correct package name if it exists in packages. for ex: fastlite is a wrapper around apsw and sqlite, lisette is a wrapper around litellm.\n",
    "   - Craft the right fts5 to get the best response.\n",
    "   - Craft a precise emb_q for semantic search if needed.\n",
    "   - top_k=5–10. Never more unless explicitly asked.\n",
    "   - wide=True if the question is vague or broad.\n",
    "   - web=True if the question seems to need web results.\n",
    "3. run_code(code: str) → MANDATORY: execute the final example before replying. If it fails, fix and retry.\n",
    "4. get_globals() → MANDATORY: call before any code that uses variables. Never clash with user namespace.\n",
    "\n",
    "RESPONSE RULES — NON-NEGOTIABLE:\n",
    "- Step 0: ALWAYS think about which tools to use and in what order. Plan your steps carefully. You can use rag(), get_globals() and run_code() multiple times.\n",
    "- Step 1 is internal thinking only. Never show it unless it contains a tool call.\n",
    "- ALWAYS call rag() first. No exceptions for \"simple\" questions.\n",
    "- if the results returned aren't sufficient to answer the question, refine your query and call rag() with `web` true again.\n",
    "- Quote the most relevant source chunk verbatim (with path comment).\n",
    "- Synthesize → minimal explanation → final runnable example.\n",
    "- Final answer MUST be a single ```python code block. Nothing after it except optional one-sentence note.\n",
    "- Judge the answer and If the answer is not good enough, research and reply with improved code.\n",
    "- Use unique variable names (e.g. _result, _df, _items, _chat) unless user explicitly reuses theirs. Make sure you do not clash with user namespace.\n",
    "- If run_code fails → fix silently and retry until it passes.\n",
    "- Keep total response ≤ 250 words.\n",
    "\n",
    "MEMORY: You remember every past example in this conversation. Reuse and refine them when relevant.\n",
    "\n",
    "FAILURE IS NOT AN OPTION.\n",
    "Be brutal. If the user’s idea is dumb, say so and give the right way.\n",
    "  '''\n",
    "xtra = {'editor-version': 'vscode/1.85.1','Copilot-Integration-Id': 'vscode-chat'}\n",
    "chat=Chat('github_copilot/claude-sonnet-4', sp, tools=[rag, websearch, run_code, get_globals])\n",
    "c=bind(chat, max_steps=8, return_all=True, max_tokens=10000,extra_headers=xtra)"
   ],
   "outputs": [],
   "execution_count": 105
  },
  {
   "cell_type": "markdown",
   "id": "b45502327d97be26",
   "metadata": {},
   "source": [
    "syntactic sugar to format the last response to show code, tool name, code executed and code result."
   ]
  },
  {
   "cell_type": "code",
   "id": "79797b67b857af76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T06:10:09.290233Z",
     "start_time": "2025-12-04T06:10:09.286486Z"
    }
   },
   "source": [
    "@patch\n",
    "def fmt_res(self:Chat, copy:bool=True):\n",
    "\t'Format the last response to show code, tool name, code executed and code result.'\n",
    "\timport pyperclip\n",
    "\tlm,tr,ltc=self.hist[-1].content, self.hist[-2] or None, getattr(self.hist[-3], 'tool_calls', None)\n",
    "\tcode = lm.split('```python')[-1].split('```')[0].strip()\n",
    "\ttn=ce=cr= None\n",
    "\tif tr and ltc: tn,ce,cr= tr['name'], json.loads(dict2obj(ltc[-1]).function.arguments)['code'], tr['content']\n",
    "\tif copy: pyperclip.copy(code); print('Code copied to clipboard!')\n",
    "\treturn AttrDict(code=code, tool_name=tn, code_executed=ce, code_result=cr)"
   ],
   "outputs": [],
   "execution_count": 106
  },
  {
   "cell_type": "markdown",
   "id": "f62b4c3f7a07a8fd",
   "metadata": {},
   "source": [
    "Let's try some code generation tasks now."
   ]
  },
  {
   "cell_type": "code",
   "id": "c018d178d87d3a09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T06:10:24.738476Z",
     "start_time": "2025-12-04T06:10:10.879871Z"
    }
   },
   "source": "r=c('parallel processing of a list in fastcore')",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[92m17:10:10 - LiteLLM:WARNING\u001B[0m: authenticator.py:99 - API key expired, refreshing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001B[0m\n",
      "\n"
     ]
    }
   ],
   "execution_count": 107
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T06:10:34.136492Z",
     "start_time": "2025-12-04T06:10:34.098402Z"
    }
   },
   "cell_type": "code",
   "source": "print(chat.fmt_res().code)",
   "id": "3a1b1acfd7e48b07",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code copied to clipboard!\n",
      "from fastcore.parallel import parallel\n",
      "\n",
      "def square(x):\n",
      "    return x ** 2\n",
      "\n",
      "_numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "_parallel_result = parallel(square, _numbers, n_workers=4)\n",
      "print(_parallel_result)  # [1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n"
     ]
    }
   ],
   "execution_count": 108
  },
  {
   "cell_type": "code",
   "id": "37ede1ead2c69348",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T06:11:06.575210Z",
     "start_time": "2025-12-04T06:10:47.899067Z"
    }
   },
   "source": "r=c('running a model inference using litellm');print(chat.fmt_res().code)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001B[0m\n",
      "\n",
      "Code copied to clipboard!\n",
      "from lisette import Chat\n",
      "\n",
      "_chat_client = Chat(model=\"gpt-4o-mini\", temp=0.7)\n",
      "_response = _chat_client(\"What is the capital of France?\")\n",
      "print(_response.choices[0].message.content)  # \"The capital of France is Paris.\"\n"
     ]
    }
   ],
   "execution_count": 109
  },
  {
   "cell_type": "code",
   "id": "12f786aee8896520",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T02:39:56.180091Z",
     "start_time": "2025-12-04T02:38:03.853807Z"
    }
   },
   "source": "r=c('how can I distill a model using model2vec and save it locally on a file to reuse later. websearch the best code embedding model from hf and use that repo.');print(chat.fmt_res().code)",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/71293/code/litesearch/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "MPS is available but PyTorch 2.9.1 has known performance regressions. Falling back to CPU. Please use a PyTorch version < 2.8.0 to enable MPS support.\n",
      "Encoding tokens: 100%|██████████| 50262/50262 [00:19<00:00, 2525.97 tokens/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code copied to clipboard!\n",
      "from model2vec.distill.distillation import distill\n",
      "from model2vec import StaticModel\n",
      "\n",
      "# Use the CodeBERT model\n",
      "hf_model = \"microsoft/codebert-base\"\n",
      "\n",
      "# Distill the encoder down to a static model (low rank pca, 256 dims)\n",
      "static_model = distill(model_name=hf_model, pca_dims=256)\n",
      "\n",
      "# Save to disk\n",
      "save_path = \"./distilled-codebert-base\"\n",
      "static_model.save_pretrained(save_path)\n",
      "\n",
      "# Reload when needed\n",
      "reloaded = StaticModel.from_pretrained(save_path)\n",
      "# Test: get embedding for code\n",
      "embedding = reloaded.encode([\"def foo():\\n    return 42\"])\n",
      "print(embedding.shape)  # (1, 256)\n"
     ]
    }
   ],
   "execution_count": 88
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Le",
   "id": "ffd670264925f3a2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T06:16:26.515430Z",
     "start_time": "2025-12-04T06:16:09.633154Z"
    }
   },
   "cell_type": "code",
   "source": "r=c('how can I compress a video using ffmpeg')",
   "id": "27efd244d9dab5e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001B[0m\n",
      "\n"
     ]
    }
   ],
   "execution_count": 110
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T06:16:34.073143Z",
     "start_time": "2025-12-04T06:16:34.027074Z"
    }
   },
   "cell_type": "code",
   "source": "print(chat.fmt_res().code)",
   "id": "588432cd369cc425",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code copied to clipboard!\n",
      "# Most common compression command - good quality/size balance\n",
      "ffmpeg -i input.mp4 -c:v libx264 -crf 24 -c:a aac -movflags +faststart output.mp4\n",
      "\n",
      "# Aggressive compression - reduce resolution + higher CRF  \n",
      "ffmpeg -i input.mp4 -vf scale=1280:720 -c:v libx264 -crf 28 -c:a aac output.mp4\n"
     ]
    }
   ],
   "execution_count": 111
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Export Notebook",
   "id": "df81135928711690"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#| hide\n",
    "from nbdev.export import nb_export; nb_export('02_tool_use.ipynb')"
   ],
   "id": "271a8459c9a60be4",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
