{
 "cells": [
  {
   "cell_type": "code",
   "id": "9ea9b9b51027a658",
   "metadata": {},
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from fastcore.all import *\n",
    "from litellm.proxy.client.cli.commands.teams import available\n",
    "from litesearch import *\n",
    "from ddgs import DDGS\n",
    "from lisette import *\n",
    "from toolslm.funccall import *\n",
    "from toolslm.md_hier import *\n",
    "import ast\n",
    "from ast import get_source_segment as gs\n",
    "import sys\n",
    "import gc\n",
    "import json\n",
    "import nbformat as nbf\n",
    "from IPython import get_ipython"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6ac0a500fa7ecd96",
   "metadata": {},
   "source": [
    "Let's set up the database and create the necessary tables. packages will store package metadata and store will store code snippets with their embeddings."
   ]
  },
  {
   "cell_type": "code",
   "id": "1f70c9bd3cc45e47",
   "metadata": {},
   "source": [
    "def mk_store(pth='code.db'):\n",
    "\t'Setup litesearch and create needed tales for code storage.'\n",
    "\tdb = setup_db(pth)\n",
    "\tstore = db.mk_store()\n",
    "\tif 'package' not in store.c: store.add_column('package', str)\n",
    "\tpackages = db.t.packages.create(name=str, version=str, summary=str, uploaded_at=float, pk=['name'], defaults=dict(uploaded_at='CURRENT_TIMESTAMP'), not_null=['name'], if_not_exists=True)\n",
    "\treturn db, store, packages"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "db,store,packages = mk_store()",
   "id": "db778ccb46e0a1bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1259a1cb621a7bee",
   "metadata": {},
   "source": [
    "Let's create a simple code ingestion pipeline which ingests functions, classes, and global variables from python packages into a litesearch database for searching. We'll be using ast module to parse python files and extract relevant code snippets."
   ]
  },
  {
   "cell_type": "code",
   "id": "47350eccaee930e6",
   "metadata": {},
   "source": [
    "def code2chunks(pkg,p):\n",
    "\ttxt=Path(p).read_text(encoding='utf-8')\n",
    "\ttree=ast.parse(txt)\n",
    "\t[setattr(c,'parent',n) for n in ast.walk(tree) for c in ast.iter_child_nodes(n)]\n",
    "\tdef n2c(n): return dict(content=gs(txt, n).strip(), metadata=p, package=pkg, uploaded_at=Path(p).stat().st_mtime)\n",
    "\tdef is_func(n): return isinstance(n,(ast.FunctionDef,ast.AsyncFunctionDef,ast.ClassDef))\n",
    "\tdef is_assign(n): return isinstance(n, (ast.Assign, ast.AnnAssign)) and n.value\n",
    "\tdef is_p_mod(n): return getattr(getattr(n,'parent',None),'__class__',None) == ast.Module\n",
    "\tdef is_global(n): return False if not is_assign(n) else True if is_p_mod(n) else False\n",
    "\tdef is_allowed(n): return is_func(n) or is_global(n)\n",
    "\treturn L(ast.walk(tree)).filter(is_allowed).map(n2c)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4f959c81966e6437",
   "metadata": {},
   "source": [
    "The main ingestion function which takes a list of package names, extracts python files from them, parses the files to extract functions, classes, and global variables, and stores them in the database."
   ]
  },
  {
   "cell_type": "code",
   "id": "145ed1d628473879",
   "metadata": {},
   "source": [
    "py_dir_skip_re=r'(^tests?$|^__pycache__$|^\\.eggs$|^\\.mypy_cache$|^\\.tox$|^examples?$|^docs?$|^build$|^dist$|^\\.git$|^\\.ipynb_checkpoints$)'\n",
    "py_file_skip_re=r'(^__init__\\.py$|^setup\\.py$|^conftest\\.py$|^test_.*\\.py$|^tests?\\.py$|^.*_test\\.py$)'\n",
    "py_glob, skip_py_glob = '*.py', '_*'\n",
    "\n",
    "def pkg2files(pkg:str,\t\t\t\t\t\t\t\t# package name\n",
    "              file_glob:str=py_glob,\t\t\t\t# file glob to match\n",
    "              skip_file_glob:str=skip_py_glob,\t\t# file glob to skip\n",
    "              skip_file_re=py_file_skip_re, \t\t# regex to skip files\n",
    "              skip_folder_re=py_dir_skip_re, \t\t# regex to skip folders\n",
    "              **kw\t\t\t\t\t\t\t\t\t# additional args to pass to globtastic\n",
    ")->L:\n",
    "\t'Return list of python files in a package excluding tests and setup files.'\n",
    "\tfrom importlib.util import find_spec as fs\n",
    "\tif not (spec:=fs(pkg)): return L()\n",
    "\treturn globtastic(Path(spec.origin).parent,file_glob=file_glob,skip_file_glob=skip_file_glob,folder_re=pkg, skip_folder_re=skip_folder_re, skip_file_re=skip_file_re, **kw)\n",
    "\n",
    "def ingest(pkgs=list):\n",
    "    def _(pkg):\n",
    "        from importlib.metadata import version as v, metadata as meta\n",
    "        if packages(select='name', where=f'name={pkg!r} and version={v(pkg)!r}'): return\n",
    "        packages.upsert(dict(name=pkg, version=v(pkg)), summary=meta(pkg)['Summary'], pk='name')\n",
    "        store.insert_all(pkg2files(pkg).map(lambda p: code2chunks(pkg,p)).concat(), upsert=True)\n",
    "    return L(pkgs).map(_)\n",
    "\n",
    "def all_user_packages():\n",
    "\tfrom importlib.metadata import distributions as dist\n",
    "\tnot_stdlib = lambda d: d.metadata.get('Author') not in ('Python', None)\n",
    "\treturn L(dist()).filter(not_stdlib).map(lambda d: d.metadata['Name'])\n",
    "\n",
    "all_user_packages()\n",
    "pl = ['fastcore', 'fastlite', 'lisette', 'toolslm', 'chonkie', 'model2vec']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "43bb5aaa1c47abfc",
   "metadata": {},
   "source": [
    "We'll use modern-bert-base onnx model for embedding the code snippets. We'll ingest the chunks first and then embed them. You can do both together. I like to separate it so that I can add more columns and test with different embedding models later. eg: `store.add_column('test_embedding', bytes)` and then `embed_docs(col='test_embedding', f=other_model.encode_document)`"
   ]
  },
  {
   "cell_type": "code",
   "id": "3c53fd17321d6be7",
   "metadata": {},
   "source": [
    "ingest(pl)\n",
    "# ingest(all_user_packages()) # alternatively ingest all user installed packages. very slow"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b8e8b4911632cf8b",
   "metadata": {},
   "source": [
    "FasstEncode is a simple ONNX based embedding model wrapper which can work with most onnx models with a huggingface tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "id": "99ebab1986a142d5",
   "metadata": {},
   "source": [
    "m1 = FastEncode(modernbert, md='models/%s' % modernbert.model)\n",
    "doc_emb, query_emb = m1.encode_document, m1.encode_query"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "61b28690924a815c",
   "metadata": {},
   "source": [
    "The main embedding function which embeds all documents in the store table using the provided embedding function."
   ]
  },
  {
   "cell_type": "code",
   "id": "46a656ff9d274b77",
   "metadata": {},
   "source": [
    "def n(col, st, reembed=False): return only(st(select='count(id) as c', where=None if reembed else f'{col} is null'))['c']\n",
    "def embed_docs(st=store, f=doc_emb, col='embedding', sz=100, reembed=False):\n",
    "\t'Embed all documents in a table using emb_f'\n",
    "\tst2fill = n(col, st, reembed)\n",
    "\twhile n(col, st, reembed)>0: # onnx models sometimes fail silently so we loop until all are done\n",
    "\t\tfor offset in range(0,st2fill,sz):\n",
    "\t\t\tbatch=st(where=f'{col} is NULL', limit=sz, offset=offset)\n",
    "\t\t\tc = [b['content'] for b in batch if b['content'].strip()]\n",
    "\t\t\tif not c: continue\n",
    "\t\t\tembs=f(c)\n",
    "\t\t\tfor e,b in zip(embs,batch): b[col] = e.tobytes()\n",
    "\t\t\tst.upsert_all(batch, pk='id')\n",
    "\t\tst2fill=n(col, st, reembed)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "n(col='embedding', st=store)",
   "id": "68f2028803d38aff",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ee61e577d30b5553",
   "metadata": {},
   "source": "embed_docs() # using modern-bert-base onnx model",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "n(col='embedding', st=store)",
   "id": "67253fefdb61961f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7a19aa2035933126",
   "metadata": {},
   "source": [
    "Some helper functions and tools for our coding assistant."
   ]
  },
  {
   "cell_type": "code",
   "id": "9fe9d5800003323f",
   "metadata": {},
   "source": [
    "@timed_cache(3600)\n",
    "def get_pkgs():\n",
    "    'Return list of ingested package names.'\n",
    "    return [p['name'] for p in packages(select='name')]\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9a4e4506bd5546e",
   "metadata": {},
   "source": [
    "def code_search(q:str,               \t# query to search\n",
    "                emb_q:str=None,     \t# query to embed. If None, use q\n",
    "\t\t\t\temb_f= query_emb, # embedding function\n",
    "                wide:bool=False,    \t# whether to use wide search\n",
    "                **kw\t\t\t\t\t# additional args to pass to db.search\n",
    "):\n",
    "\t'Code search through the database to find relevant code snippets.'\n",
    "\temb = emb_f(emb_q if emb_q else q)\n",
    "\tavailable = L(set(q.split(' ')).intersection(get_pkgs()))\n",
    "\twh = f'package in ({','.join(available.map(repr))})' if available else None\n",
    "\tkw['where'] = wh if 'where' not in kw else f\"({kw['where']}) AND {wh}\" if wh else kw['where']\n",
    "\treturn db.search(pre(q, wide=wide), emb.tobytes(), **kw)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "16cd11b65addf1bb",
   "metadata": {},
   "source": [
    "def websearch(q: str,               # query to search\n",
    "              top_k: int = 10,      # number of top results to return\n",
    "              ):\n",
    "\t'Web search results reranked with flashrank'\n",
    "\tres = dict2obj(DDGS().text(q, max_results=top_k))\n",
    "\treturn json.dumps([dict(text=r.body, id=r.href, meta=dict(title=r.title)) for r in res])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8fbd916339362c31",
   "metadata": {},
   "source": [
    "The main RAG function which first searches the code database and then optionally performs a web search if needed. You can also use models that support web search directly wiuth lisette(litellm wrapper) instead of the web function."
   ]
  },
  {
   "cell_type": "code",
   "id": "6abce742b9a15add",
   "metadata": {},
   "source": [
    "def rag(q: str,\t# query string\n",
    "        emb_q: str = None, # query to embed. If None, use q\n",
    "        top_k: int = 5, # number of top results to return\n",
    "        wide: bool = False, # whether to use wide search\n",
    "        web: bool = False # whether to include web search results\n",
    ") -> str:\n",
    "    'Search indexed code for relevant chunks. Returns structured results.'\n",
    "    r = code_search(q, emb_q=emb_q if emb_q else q, lim=top_k, columns=['content', 'metadata'], wide=wide)\n",
    "    if web: r += websearch(q, top_k=top_k // 2)  # Balance\n",
    "    return json.dumps(dict(query=q, results=r, top_k=top_k))  # Or dict if no Pydantic"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ee23615d40e2aaf4",
   "metadata": {},
   "source": [
    "def run_code(code:str # code to run\n",
    "             )->str:\n",
    "\t'''Run code in python interpreter'''\n",
    "\treturn python(code, globals())\n",
    "\n",
    "def get_globals():\n",
    "\t'Return current global variables.'\n",
    "\treturn globals().keys()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "99f96b2ce448a972",
   "metadata": {},
   "source": [
    "def get_live_nb() -> Path:\n",
    "\t'''Returns the live notebook's path you're currently running in.'''\n",
    "\tip = get_ipython()\n",
    "\tif not ip: return None\n",
    "\topt = globtastic(Path.cwd(), file_glob='*.ipynb').sorted(key=os.path.getmtime, reverse=True)\n",
    "\treturn opt[0] if opt else None\n",
    "\n",
    "def ai(code: str, # code to insert\n",
    "       nb_path: Path | str | None = None # notebook path to use\n",
    "       ):\n",
    "\t'AI → inserts the given code string into the very next code cell.'\n",
    "\tif not nb_path: nb_path = get_live_nb() or 'Untitled.ipynb'\n",
    "\tnb, ip = nbf.read(nb_path, as_version=4), get_ipython()\n",
    "\tidx = L(nb.cells).argfirst(lambda c: c.cell_type == 'code' and c.execution_count == ip.execution_count) or -1\n",
    "\tins_idx = idx + 1 if idx >= 0 else len(nb.cells)\n",
    "\tnw_c = nbf.v4.new_code_cell(source=code.strip() + \"\\n\")\n",
    "\tnb.cells.insert(ins_idx, nw_c)\n",
    "\tnbf.write(nb, nb_path)\n",
    "\n",
    "def ins(code: str):\n",
    "\t'Insert code into the next cell of the current notebook.'\n",
    "\tip = get_ipython()\n",
    "\tif not ip: return\n",
    "\tip.set_next_input(code.strip() + \"\\n\", replace=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c0737f785e25504",
   "metadata": {},
   "source": [
    "sp=f'''You are a razor-sharp Python coding assistant with perfect knowledge of fastcore, fastlite, lisette, toolslm, chonkie, model2vec and every package indexed in your RAG database.\n",
    "\n",
    "Your ONLY job: answer with concise, working, copy-pasteable code. No essays. No apologies.\n",
    "Available packages: {packages(select='name,summary')}.\n",
    "TOOLS (in exact order you MUST follow):\n",
    "1. rag(q: str, emb_q: str | None, top_k: int = 7) → ALWAYS call this first on every code question.\n",
    "   - Use natural language q for FTS5, Do not pass the user query as is to q. Clean the query first. Add package names if teh query matches any ingested package. Use the summary of packages to help you.\n",
    "   - Craft the right fts5 to get the best response.\n",
    "   - Craft a precise emb_q for semantic search if needed.\n",
    "   - top_k=5–10. Never more unless explicitly asked.\n",
    "2. websearch(q) → ONLY if rag returned <3 useful chunks or the question is about unindexed packages.\n",
    "3. run_code(code: str) → MANDATORY: execute the final example before replying. If it fails, fix and retry.\n",
    "4. get_globals() → MANDATORY: call before any code that uses variables. Never clash with user namespace.\n",
    "\n",
    "RESPONSE RULES — NON-NEGOTIABLE:\n",
    "- Step 1 is internal thinking only. Never show it unless it contains a tool call.\n",
    "- ALWAYS call rag() first. No exceptions for \"simple\" questions.\n",
    "- Quote the most relevant source chunk verbatim (with path comment).\n",
    "- Synthesize → minimal explanation → final runnable example.\n",
    "- Final answer MUST be a single ```python code block. Nothing after it except optional one-sentence note.\n",
    "- Judge the answer and If the answer is not good enough, research and reply with improved code.\n",
    "- Use unique variable names (e.g. _result, _df, _items, _chat) unless user explicitly reuses theirs. Make sure you do not clash with user namespace.\n",
    "- If run_code fails → fix silently and retry until it passes.\n",
    "- Keep total response ≤ 250 words.\n",
    "\n",
    "MEMORY: You remember every past example in this conversation. Reuse and refine them when relevant.\n",
    "\n",
    "FAILURE IS NOT AN OPTION.\n",
    "Be brutal. If the user’s idea is dumb, say so and give the right way.\n",
    "  '''"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ea2ceca8e7b2a7a8",
   "metadata": {},
   "source": [
    "chat=Chat('gpt-4.1', sp, tools=[rag, websearch, run_code, get_globals])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "861a477b2c0ffbee",
   "metadata": {},
   "source": [
    "c=bind(chat, max_steps=8, return_all=True, max_tokens=10000)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b45502327d97be26",
   "metadata": {},
   "source": [
    "syntactic sugar to format the last response to show code, tool name, code executed and code result."
   ]
  },
  {
   "cell_type": "code",
   "id": "79797b67b857af76",
   "metadata": {},
   "source": [
    "@patch\n",
    "def fmt_res(self:Chat, copy:bool=True):\n",
    "\t'Format the last response to show code, tool name, code executed and code result.'\n",
    "\timport pyperclip\n",
    "\tlm,tr,ltc=self.hist[-1].content, self.hist[-2], self.hist[-3].tool_calls\n",
    "\tcode = lm.split('```python')[-1].split('```')[0].strip()\n",
    "\ttn,ce,cr= tr['name'], json.loads(dict2obj(ltc[-1]).function.arguments)['code'], tr['content']\n",
    "\tif copy: pyperclip.copy(code); print('Code copied to clipboard!')\n",
    "\treturn AttrDict(code=code, tool_name=tn, code_executed=ce, code_result=cr)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f62b4c3f7a07a8fd",
   "metadata": {},
   "source": [
    "Let's try some code generation tasks now."
   ]
  },
  {
   "cell_type": "code",
   "id": "3a1b1acfd7e48b07",
   "metadata": {},
   "source": [
    "r=c('parallel processing of a list in fastcore');print(chat.fmt_res().code)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "37ede1ead2c69348",
   "metadata": {},
   "source": [
    "r=c('running a model inference using litellm');print(chat.fmt_res().code)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8eaf405f58eaf26e",
   "metadata": {},
   "source": [
    "code = '''\n",
    " for i, cell in enumerate(nb.cells):\n",
    "        if cell.cell_type == 'code' and cell.source.strip() == src.strip():\n",
    "            idx = i\n",
    "            break'''\n",
    "r=c(['I want to make this code more efficient using fastcore utilities. Rewrite it.', code]); print(chat.fmt_res().code)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b2f75a44e3e54b4",
   "metadata": {},
   "source": [
    "We can use the `ai` function to insert code into the next cell of the current notebook and run it if we want to."
   ]
  },
  {
   "cell_type": "code",
   "id": "95a3845c0f8515ad",
   "metadata": {},
   "source": [
    "ai(chat.fmt_res().code)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "186868665eaf040d",
   "metadata": {},
   "source": [
    "This time let's actually ask it to do things in the workspace. You're basically putting the agent to work on pointed tasks."
   ]
  },
  {
   "cell_type": "code",
   "id": "12f786aee8896520",
   "metadata": {},
   "source": [
    "r=c('how can I distill a model using model2vec and save it locally on a file to reuse later. take the example of nomic-ai/modernbert-embed-base');print(chat.fmt_res().code)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "18a07fc21c214361",
   "metadata": {},
   "source": [
    "ai(chat.fmt_res().code)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "10bd2763",
   "metadata": {},
   "source": [
    "print(\"Hello from AI inserted code!\")\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
