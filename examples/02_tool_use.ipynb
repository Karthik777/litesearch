{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "---\n",
    "description: Examples to show how to use litesearch to build a coding assistant with RAG\n",
    "  capabilities\n",
    "title: tool-use\n",
    "\n",
    "---"
   ],
   "id": "b3c814e167123f58"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "#| default_exp code-assistant",
   "id": "bcf6aae51dd40592",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Setup",
   "id": "c5af511907d5579a"
  },
  {
   "cell_type": "code",
   "id": "9ea9b9b51027a658",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#| export\n",
    "from ddgs import DDGS\n",
    "from fastcore.all import *\n",
    "from litesearch import *\n",
    "from IPython import get_ipython\n",
    "import json\n",
    "from lisette import *\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from toolslm.funccall import *"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6ac0a500fa7ecd96",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Let's set up the database and create the necessary tables. packages will store package metadata and store will store code snippets with their embeddings."
   ]
  },
  {
   "cell_type": "code",
   "id": "1f70c9bd3cc45e47",
   "metadata": {},
   "source": [
    "#| export\n",
    "def mk_store(pth='code.db'):\n",
    "\t'Setup litesearch and create needed tales for code storage.'\n",
    "\tdb = setup_db(pth)\n",
    "\tstore = db.mk_store()\n",
    "\tif 'package' not in store.c: store.add_column('package', str)\n",
    "\tpackages = db.t.packages.create(name=str, version=str, summary=str, uploaded_at=float, pk=['name'], defaults=dict(uploaded_at='CURRENT_TIMESTAMP'), not_null=['name'], if_not_exists=True)\n",
    "\treturn db, store, packages"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "db778ccb46e0a1bb",
   "metadata": {},
   "source": [
    "db,store,packages = mk_store()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(store(where='embedding is NULL'))",
   "id": "b36c40a49ac4f58a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Ingestion Pipeline",
   "id": "613af917d389bf5f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Chunk Packages",
   "id": "e0c4ef0570aa961d"
  },
  {
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "cell_type": "markdown",
   "source": "Let's create a simple code ingestion pipeline which ingests functions, classes, and global variables from python packages into a litesearch database for searching. We'll be using ast module to parse python files and extract relevant code snippets.",
   "id": "1259a1cb621a7bee"
  },
  {
   "cell_type": "code",
   "id": "47350eccaee930e6",
   "metadata": {},
   "source": [
    "#| export\n",
    "def code2chunks(pkg,p):\n",
    "\timport ast\n",
    "\tfrom ast import get_source_segment as gs\n",
    "\ttxt=Path(p).read_text(encoding='utf-8')\n",
    "\ttree=ast.parse(txt)\n",
    "\t[setattr(c,'parent',n) for n in ast.walk(tree) for c in ast.iter_child_nodes(n)]\n",
    "\tdef n2c(n): return dict(content=gs(txt, n).strip(), metadata=p, package=pkg, uploaded_at=Path(p).stat().st_mtime)\n",
    "\tdef is_func(n): return isinstance(n,(ast.FunctionDef,ast.AsyncFunctionDef,ast.ClassDef))\n",
    "\tdef is_assign(n): return isinstance(n, (ast.Assign, ast.AnnAssign)) and n.value\n",
    "\tdef is_p_mod(n): return getattr(getattr(n,'parent',None),'__class__',None) == ast.Module\n",
    "\tdef is_global(n): return False if not is_assign(n) else True if is_p_mod(n) else False\n",
    "\tdef is_allowed(n): return is_func(n) or is_global(n)\n",
    "\treturn L(ast.walk(tree)).filter(is_allowed).map(n2c)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4f959c81966e6437",
   "metadata": {},
   "source": [
    "The main ingestion function which takes a list of package names, extracts python files from them, parses the files to extract functions, classes, and global variables, and stores them in the database."
   ]
  },
  {
   "cell_type": "code",
   "id": "145ed1d628473879",
   "metadata": {},
   "source": [
    "#| export\n",
    "py_dir_skip_re=r'(^tests?$|^__pycache__$|^\\.eggs$|^\\.mypy_cache$|^\\.tox$|^examples?$|^docs?$|^build$|^dist$|^\\.git$|^\\.ipynb_checkpoints$)'\n",
    "py_file_skip_re=r'(^__init__\\.py$|^setup\\.py$|^conftest\\.py$|^test_.*\\.py$|^tests?\\.py$|^.*_test\\.py$)'\n",
    "py_glob, skip_py_glob = '*.py', '_*'\n",
    "\n",
    "@delegates(globtastic)\n",
    "def pkg2files(pkg:str,\t\t\t\t\t\t\t\t# package name\n",
    "              file_glob:str=py_glob,\t\t\t\t# file glob to match\n",
    "              skip_file_glob:str=skip_py_glob,\t\t# file glob to skip\n",
    "              skip_file_re=py_file_skip_re, \t\t# regex to skip files\n",
    "              skip_folder_re=py_dir_skip_re, \t\t# regex to skip folders\n",
    "\t\t\t  **kwargs\t\t\t\t\t\t\t\t# additional args to pass to globtastic\n",
    ")->L:\n",
    "\t'Return list of python files in a package excluding tests and setup files.'\n",
    "\treturn globtastic(Path(spec.origin).parent,file_glob=file_glob,skip_file_glob=skip_file_glob,\n",
    "        folder_re=pkg, skip_folder_re=skip_folder_re, skip_file_re=skip_file_re, **kwargs)\n",
    "\n",
    "def ingest(pkgs=list):\n",
    "    def _(pkg):\n",
    "        from importlib.metadata import version as v, metadata as meta\n",
    "        if packages(select='name', where=f'name={pkg!r} and version={v(pkg)!r}'): return\n",
    "        packages.upsert(dict(name=pkg, version=v(pkg)), summary=meta(pkg)['Summary'], pk='name')\n",
    "        store.insert_all(pkg2files(pkg).map(lambda p: code2chunks(pkg,p)).concat(), upsert=True)\n",
    "    return L(pkgs).map(_)\n",
    "\n",
    "def pkgs(nms:list=None, full=False):\n",
    "\tfrom importlib.util import find_spec as fs\n",
    "\tfrom importlib.metadata import distributions as dists, distribution as dist\n",
    "\tnot_stdlib = lambda d: d.metadata.get('Author-email') not in ('Python', None)\n",
    "\tpkgs = L(dists()) if full else L(nms).filter(fs).map(dist)\n",
    "\treturn pkgs.filter(not_stdlib).map(lambda d: d.metadata['Name'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "43bb5aaa1c47abfc",
   "metadata": {},
   "source": [
    "We'll use modern-bert-base onnx model for embedding the code snippets. We'll ingest the chunks first and then embed them. You can do both together. I like to separate it so that I can add more columns and test with different embedding models later. eg: `store.add_column('test_embedding', bytes)` and then `embed_docs(col='test_embedding', f=other_model.encode_document)`"
   ]
  },
  {
   "cell_type": "code",
   "id": "3c53fd17321d6be7",
   "metadata": {},
   "source": [
    "pl = ['fastcore', 'fastlite', 'lisette', 'toolslm', 'chonkie', 'model2vec']\n",
    "ingest(pkgs(pl))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Encoding Setup",
   "id": "5f284c25d5984d24"
  },
  {
   "cell_type": "markdown",
   "id": "b8e8b4911632cf8b",
   "metadata": {},
   "source": [
    "FasstEncode is a simple ONNX based embedding model wrapper which can work with most onnx models with a huggingface tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "id": "99ebab1986a142d5",
   "metadata": {},
   "source": [
    "m1 = FastEncode(modernbert, md='models/%s' % modernbert.model)\n",
    "doc_emb, query_emb = m1.encode_document, m1.encode_query"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Embedding code chunks",
   "id": "4133a1aeb35c867e"
  },
  {
   "cell_type": "markdown",
   "id": "61b28690924a815c",
   "metadata": {},
   "source": [
    "The main embedding function which embeds all documents in the store table using the provided embedding function."
   ]
  },
  {
   "cell_type": "code",
   "id": "46a656ff9d274b77",
   "metadata": {},
   "source": [
    "#| export\n",
    "def embed_docs(st=None, f=None, col='embedding', sz=100, reembed=False):\n",
    "\t'Embed all documents in a table using emb_f'\n",
    "\tif not (st and f): return\n",
    "\tn = len(st(where='embedding is NULL' if reembed else None))\n",
    "\tfor offset in range(0, n, sz):\n",
    "\t\tbatch=st(where=f'{col} is NULL', limit=sz, offset=offset)\n",
    "\t\tc = [b['content'] for b in batch if b['content'].strip()]\n",
    "\t\tif not c: continue\n",
    "\t\tfor e,b in zip(f(c),batch): b[col] = e.tobytes()\n",
    "\t\tst.upsert_all(batch, pk='id')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "68f2028803d38aff",
   "metadata": {},
   "source": [
    "log = lambda: print('Docs without embedding: ', len(store(select='id',where='embedding is NULL')),'\\n','-'*20)\n",
    "log();embed_docs(st=store, f=doc_emb);log() # using modern-bert-base onnx model. let's make sure all chunks are embedded"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Coding Assistant",
   "id": "b25ab5c0337b8a96"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Coding Assistant Tools",
   "id": "1408c886b4dfebdd"
  },
  {
   "cell_type": "markdown",
   "id": "7a19aa2035933126",
   "metadata": {},
   "source": [
    "Some helper functions and tools for our coding assistant."
   ]
  },
  {
   "cell_type": "code",
   "id": "372aec612a248be6",
   "metadata": {},
   "source": [
    "#| export\n",
    "@timed_cache(3600)\n",
    "def get_pkgs():\n",
    "    'Return list of ingested package names.'\n",
    "    return [p['name'] for p in packages(select='name')]\n",
    "\n",
    "def code_search(q:str,               \t# query to search\n",
    "                emb_q:str=None,     \t# query to embed. If None, use q\n",
    "\t\t\t\temb_f= query_emb, # embedding function\n",
    "                wide:bool=False,    \t# whether to use wide search\n",
    "                **kw\t\t\t\t\t# additional args to pass to db.search\n",
    "):\n",
    "\t'Code search through the database to find relevant code snippets.'\n",
    "\temb = emb_f(emb_q if emb_q else q)\n",
    "\tavailable = L(set(q.split(' ')).intersection(get_pkgs()))\n",
    "\twh = f'package in ({','.join(available.map(repr))})' if available else None\n",
    "\tkw['where'] = wh if 'where' not in kw else f\"({kw['where']}) AND {wh}\" if wh else kw['where']\n",
    "\treturn db.search(pre(q, wide=wide), emb.tobytes(), **kw)\n",
    "\n",
    "def websearch(q: str,               # query to search\n",
    "              top_k: int = 10,      # number of top results to return\n",
    "              ):\n",
    "\t'Web search results reranked with flashrank'\n",
    "\tres = dict2obj(DDGS().text(q, max_results=top_k))\n",
    "\treturn json.dumps([dict(text=r.body, id=r.href, meta=dict(title=r.title)) for r in res])\n",
    "\n",
    "def rag(q: str,\t# query string\n",
    "        emb_q: str = None, # query to embed. If None, use q\n",
    "        top_k: int = 5, # number of top results to return\n",
    "        wide: bool = False, # whether to use wide search\n",
    "        web: bool = False # whether to include web search results\n",
    ") -> str:\n",
    "    'Search indexed code for relevant chunks. Returns structured results.'\n",
    "    r = code_search(q, emb_q=emb_q if emb_q else q, lim=top_k, columns=['content', 'metadata'], wide=wide)\n",
    "    if web: r += websearch(q, top_k=top_k // 2)  # Balance\n",
    "    return json.dumps(dict(query=q, results=r, top_k=top_k))  # Or dict if no Pydantic\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Code Execution Tool",
   "id": "d0cfbbdbd615ec20"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#| export\n",
    "def run_code(code:str, \t  # code to run\n",
    "             strict=True # whether to run in strict mode\n",
    "             )->str:\n",
    "\t'''Run code in python interpreter'''\n",
    "\tif not strict: python(code, globals())\n",
    "\treturn python(code)\n",
    "\n",
    "def get_globals():\n",
    "\t'Return current global variables.'\n",
    "\treturn globals().keys()"
   ],
   "id": "9fe9d5800003323f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Syntactic Sugar",
   "id": "dfec05b3607063dd"
  },
  {
   "cell_type": "code",
   "id": "99f96b2ce448a972",
   "metadata": {},
   "source": [
    "def ins(code: str):\n",
    "\t'Insert code into the next cell of the current notebook.'\n",
    "\tip = get_ipython()\n",
    "\tif not ip: return\n",
    "\tip.set_next_input(code.strip() + \"\\n\", replace=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## LLM Chat with Tools Setup. Let's not call it an Agent. Maybe an Augmented LLM Chat?",
   "id": "235f8986d76deae9"
  },
  {
   "cell_type": "code",
   "id": "c0737f785e25504",
   "metadata": {},
   "source": [
    "#| export\n",
    "sp= f'''You are a razor-sharp Python coding assistant with perfect knowledge of fastcore, fastlite, lisette, toolslm, chonkie, model2vec and every package indexed in your RAG database.\n",
    "\n",
    "Your ONLY job: answer with concise, working, copy-pasteable code. No essays. No apologies.\n",
    "Available packages: {packages(select='name,summary')}.\n",
    "TOOLS (in exact order you MUST follow):\n",
    "1. rag(q: str, emb_q: str | None, top_k: int = 7) → ALWAYS call this first on every code question.\n",
    "   - Use natural language q for FTS5, Do not pass the user query as is to q. Clean the query first. Add package names if teh query matches any ingested package. Use the summary of packages to help you.\n",
    "   - Craft the right fts5 to get the best response.\n",
    "   - Craft a precise emb_q for semantic search if needed.\n",
    "   - top_k=5–10. Never more unless explicitly asked.\n",
    "2. websearch(q) → ONLY if rag returned <3 useful chunks or the question is about unindexed packages.\n",
    "3. run_code(code: str) → MANDATORY: execute the final example before replying. If it fails, fix and retry.\n",
    "4. get_globals() → MANDATORY: call before any code that uses variables. Never clash with user namespace.\n",
    "\n",
    "RESPONSE RULES — NON-NEGOTIABLE:\n",
    "- Step 1 is internal thinking only. Never show it unless it contains a tool call.\n",
    "- ALWAYS call rag() first. No exceptions for \"simple\" questions.\n",
    "- Quote the most relevant source chunk verbatim (with path comment).\n",
    "- Synthesize → minimal explanation → final runnable example.\n",
    "- Final answer MUST be a single ```python code block. Nothing after it except optional one-sentence note.\n",
    "- Judge the answer and If the answer is not good enough, research and reply with improved code.\n",
    "- Use unique variable names (e.g. _result, _df, _items, _chat) unless user explicitly reuses theirs. Make sure you do not clash with user namespace.\n",
    "- If run_code fails → fix silently and retry until it passes.\n",
    "- Keep total response ≤ 250 words.\n",
    "\n",
    "MEMORY: You remember every past example in this conversation. Reuse and refine them when relevant.\n",
    "\n",
    "FAILURE IS NOT AN OPTION.\n",
    "Be brutal. If the user’s idea is dumb, say so and give the right way.\n",
    "  '''\n",
    "chat=Chat('gpt-4.1', sp, tools=[rag, websearch, run_code, get_globals])\n",
    "c=bind(chat, max_steps=8, return_all=True, max_tokens=10000)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b45502327d97be26",
   "metadata": {},
   "source": [
    "syntactic sugar to format the last response to show code, tool name, code executed and code result."
   ]
  },
  {
   "cell_type": "code",
   "id": "79797b67b857af76",
   "metadata": {},
   "source": [
    "@patch\n",
    "def fmt_res(self:Chat, copy:bool=True):\n",
    "\t'Format the last response to show code, tool name, code executed and code result.'\n",
    "\timport pyperclip\n",
    "\tlm,tr,ltc=self.hist[-1].content, self.hist[-2], self.hist[-3].tool_calls\n",
    "\tcode = lm.split('```python')[-1].split('```')[0].strip()\n",
    "\ttn,ce,cr= tr['name'], json.loads(dict2obj(ltc[-1]).function.arguments)['code'], tr['content']\n",
    "\tif copy: pyperclip.copy(code); print('Code copied to clipboard!')\n",
    "\treturn AttrDict(code=code, tool_name=tn, code_executed=ce, code_result=cr)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f62b4c3f7a07a8fd",
   "metadata": {},
   "source": [
    "Let's try some code generation tasks now."
   ]
  },
  {
   "cell_type": "code",
   "id": "3a1b1acfd7e48b07",
   "metadata": {},
   "source": [
    "r=c('parallel processing of a list in fastcore');print(chat.fmt_res().code)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "37ede1ead2c69348",
   "metadata": {},
   "source": [
    "r=c('running a model inference using litellm');print(chat.fmt_res().code)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8eaf405f58eaf26e",
   "metadata": {},
   "source": [
    "code = '''\n",
    " for i, cell in enumerate(nb.cells):\n",
    "        if cell.cell_type == 'code' and cell.source.strip() == src.strip():\n",
    "            idx = i\n",
    "            break'''\n",
    "r=c(['I want to make this code more efficient using fastcore utilities. Rewrite it.', code]); print(chat.fmt_res().code)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b2f75a44e3e54b4",
   "metadata": {},
   "source": [
    "We can use the `ai` function to insert code into the next cell of the current notebook and run it if we want to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186868665eaf040d",
   "metadata": {},
   "source": [
    "This time let's actually ask it to do things in the workspace. You're basically putting the agent to work on pointed tasks."
   ]
  },
  {
   "cell_type": "code",
   "id": "12f786aee8896520",
   "metadata": {},
   "source": [
    "r=c('how can I distill a model using model2vec and save it locally on a file to reuse later. take the example of nomic-ai/modernbert-embed-base');print(chat.fmt_res().code)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Export Notebook",
   "id": "df81135928711690"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#| hide\n",
    "from nbdev.export import nb_export; nb_export('02_tool_use.ipynb')"
   ],
   "id": "271a8459c9a60be4",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
