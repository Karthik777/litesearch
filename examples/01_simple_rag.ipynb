{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Simple RAG with Thedu\n",
    "> We will build a simple rag with thedu. Do not be deceived. We are doing a whole bunch of heavylifting under the hood with very little code."
   ],
   "id": "94f1234b99572d4e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T02:47:04.104742Z",
     "start_time": "2025-11-09T02:47:03.597766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#| export\n",
    "from fastcore.all import *\n",
    "from fastlite import *\n",
    "import numpy as np\n",
    "import re\n",
    "from selectolax.parser import HTMLParser\n",
    "from thedu import *\n",
    "from yake import KeywordExtractor"
   ],
   "id": "3edd79a6c9ca938",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's set the db up. This db has usearch loaded. So, you can run cosine distance calculations using simd(means fast, real fast)",
   "id": "5e79b20a43980d79"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T02:47:05.403587Z",
     "start_time": "2025-11-09T02:47:05.386052Z"
    }
   },
   "cell_type": "code",
   "source": "db:Database=setup_db('breugel.db')",
   "id": "41c065b00f3db9a1",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T02:47:06.581245Z",
     "start_time": "2025-11-09T02:47:06.576355Z"
    }
   },
   "cell_type": "code",
   "source": "db.q('select distance_cosine_f16(:vec1,:vec2)', dict(vec1=np.ones(512, np.float16).tobytes(), vec2=np.zeros(512, np.float16).tobytes()))",
   "id": "7e0193d25606a505",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'distance_cosine_f16(:vec1,:vec2)': 1.0}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "There are way more functions you can run now. Checkout: https://unum-cloud.github.io/USearch/sqlite/index.html",
   "id": "9a1b93c61a890038"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Ingest a PDF document\n",
    "> We will ingest a sample PDF document from Bruegel.\n",
    "> We will read the PDF document using `read_pdf` function from thedu.ingest module.\n",
    "> We will then scrape the urls from the pdf and then recursively get all pdf's off of those links and ingest them as well.'"
   ],
   "id": "bbe74532d19fe55b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T02:47:08.698630Z",
     "start_time": "2025-11-09T02:47:08.689057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#| export\n",
    "class BruegelDataset:\n",
    "    ''' Dataset for Bruegel PDF documents.'''\n",
    "    URL = 'https://www.bruegel.org/system/files/2024-06/Bruegel_factsheet_2024_0.pdf'\n",
    "    URI_SCHEMA = r'^http://data\\.europa\\.eu/eli/(?P<typedoc>[^/]+)/(?P<year>\\d{4})/(?P<natural_number>\\d+)/(?P<date>\\d{4}-\\d{2}-\\d{2})/(?P<lang>[a-z]{2,3})/pdfa2a$'\n",
    "    def __init__(self, dest:Path=Path('bruegel_dataset')):\n",
    "        self.dest = dest\n",
    "        self.pdfs = self()\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _is_pdf_link(l:str): return l.strip() and (l.lower().endswith('.pdf') or 'pdf' in l.lower())\n",
    "    @staticmethod\n",
    "    def url2name(url: str) -> str | None:\n",
    "        if re.match(BruegelDataset.URI_SCHEMA, url):\n",
    "            m = re.match(BruegelDataset.URI_SCHEMA, url)\n",
    "            return f\"{m['typedoc']}_{m['year']}_{m['natural_number']}_{m['date']}_{m['lang']}.pdf\"\n",
    "        return url.split('/')[-1] if url.split('/')[-1] != '' else url.rstrip('/').split('/')[-1]+'.html'\n",
    "\n",
    "    def _get_meta(self, r):\n",
    "        try:\n",
    "            meta = HTMLParser(r).tags('meta')\n",
    "            nodes = L([dict2obj(m.attributes) for m in meta]).filter(\n",
    "                lambda m: 'about' in m\n",
    "            ).filter(\n",
    "                lambda m: ('property' in m and m['property'].lower() in ['eli:is_embodied_by','eli:title'])\n",
    "            ).filter(\n",
    "                lambda m: ('resource' in m and 'pdfa2a' in m['resource'].lower()) or 'content' in m\n",
    "            ).groupby('about')\n",
    "            for k in nodes: nodes[k] = merge(*nodes[k])\n",
    "            return L([(nodes[n]['resource'], nodes[n]['content']) for n in nodes])\n",
    "        except: pass\n",
    "\n",
    "\n",
    "    def read_link(self, l, dest=None):\n",
    "        try:\n",
    "\t        p = self.save_pdf(l, dest)\n",
    "\t        return p.read_text()\n",
    "        except: return ''\n",
    "\n",
    "    def save_pdf(self, l:str, dest:Path=None) -> Path | None:\n",
    "        try:\n",
    "            if not l : return None\n",
    "            if not dest: dest = self.dest\n",
    "            p = dest / self.url2name(l)\n",
    "            if not (p.exists() and p.stat().st_size > 1024): p = urlsave(l,p)\n",
    "            return p\n",
    "        except Exception as ex: print(ex); return None\n",
    "\n",
    "    def __call__(self) -> L:\n",
    "        '''Make a dataset of documents from a pdf url and all linked pdfs.'''\n",
    "\n",
    "        def get_linked_pdfs(doc, filter=True):\n",
    "            links = doc.map(lambda p: p.links.map(lambda l: l.uri)).concat().unique()\n",
    "            if filter: links = links.filter(self._is_pdf_link)\n",
    "            return links\n",
    "\n",
    "        pth = self.dest / self.url2name(self.URL)\n",
    "        if not pth.exists(): pth = urlsave(self.URL, self.dest / self.url2name(self.URL))\n",
    "        main_doc = read_pdf(pth)\n",
    "        pdf_lp = Path(self.dest / 'pdf_links')\n",
    "        if not pdf_lp.exists():\n",
    "            links = get_linked_pdfs(main_doc, filter=False)\n",
    "            print(f'Found {len(links)} linked pdfs.')\n",
    "            all_c = parallel(self.read_link, links, threadpool=True, dest=self.dest/'links')\n",
    "            pdf_links = parallel(self._get_meta, all_c, threadpool=True).concat().unique()\n",
    "            print(f'Found {len(pdf_links)} external pdf links.')\n",
    "            pdf_lp.mk_write('\\n'.join([f'{l[0]},{l[1]}' for l in pdf_links]))\n",
    "        url2tit = {l.split(',')[0]: l.split(',')[1] for l in pdf_lp.readlines()}\n",
    "        pdf_list = L([l.split(',')[0] for l in pdf_lp.readlines()])\n",
    "        name2url = {self.url2name(l): l for l in pdf_list}\n",
    "        pdf_set = set(pdf_list.map(self.url2name))\n",
    "        downloaded_pdfs = globtastic(self.dest / 'pdfs', file_glob='*.pdf', func=Path).filter(lambda m: m.stat().st_size > 1024).map(lambda p: p.name)\n",
    "        fetch_list = [name2url.get(d) for d in pdf_set.difference(downloaded_pdfs)]\n",
    "        if fetch_list:\n",
    "            print(f'Downloading {len(fetch_list)} new pdfs...')\n",
    "            parallel(self.save_pdf, fetch_list, dest=self.dest/'pdfs')\n",
    "        return globtastic(self.dest / 'pdfs', file_glob='*.pdf', func=Path).map(lambda m: AttrDict(path=m, title=url2tit[name2url[m.name]]))"
   ],
   "id": "fe6fbcae5619ee36",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T02:47:10.388735Z",
     "start_time": "2025-11-09T02:47:10.147407Z"
    }
   },
   "cell_type": "code",
   "source": "b = BruegelDataset()",
   "id": "efb14b2e333b565d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 1 new pdfs...\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Patching is a great way to add functionalities to existing classes. It works on all python classes\n",
    "Here we are modifying the title in metadata with a better value. We get it as part of the breugel web scraping."
   ],
   "id": "3a0b1695a0351fdf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T09:54:57.838963Z",
     "start_time": "2025-11-08T09:54:57.836083Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#| export\n",
    "@patch\n",
    "def read_pdfs(self:BruegelDataset, fn=noop):\n",
    "    def update_meta(p):\n",
    "        if not fn(p): return None\n",
    "        d = read_pdf(p.path)\n",
    "        d['name'] = d.metadata.title\n",
    "        d.metadata.title = p.title\n",
    "        return d\n",
    "    return maps(update_meta, self.pdfs)"
   ],
   "id": "8f65aa86ce717748",
   "outputs": [],
   "execution_count": 126
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's load the dataset. it takes a bit the first time around as it downloads about 800 pdfs. check your examples folder for `breugel_dataset`",
   "id": "d16c94b3bf6720d5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T05:11:18.862597Z",
     "start_time": "2025-11-08T05:11:18.860758Z"
    }
   },
   "cell_type": "code",
   "source": "print('no. of pdfs: ', len(b.pdfs))",
   "id": "d7fbb25c835453a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of pdfs:  817\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's read the first pdf.",
   "id": "f00f128135c898ea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T09:56:27.635879Z",
     "start_time": "2025-11-08T09:56:27.633438Z"
    }
   },
   "cell_type": "code",
   "source": "doc = first(b.read_pdfs(lambda p: p.path.stem.endswith('_eng')))",
   "id": "6d2e7d36b3ca16b5",
   "outputs": [],
   "execution_count": 128
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now let's load the pdfs into the db. The db is already patched with a whole bunch of syntactic sugars. Checkout `thedu.data` for more info",
   "id": "4397b806a3463cd2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "db.pdf_ingest(first(b.read_pdfs()))",
   "id": "c671586761f097b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The above will work, but will load a new chunking pipe everytime. An efficient way is to get the default pipe of create your own. We use chonkie for chunking and embedding. To reduce the number of documents we're also filtering out non english docs. Checkout their docs for more information: https://docs.chonkie.ai/oss",
   "id": "95fbdd854901f55b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pipe = pdf_pipe()\n",
    "only_eng = lambda p: p.path.stem.endswith('_eng')\n",
    "[db.pdf_ingest(p, pipe) for p in b.read_pdfs(fn=only_eng) if p]"
   ],
   "id": "f4867eb74318b695",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This can be made even faster with chonie pipelines where you can pass a bunch of texts together and chunk all docs in batches first and push them to db. Checkout https://docs.chonkie.ai/oss/pipelines",
   "id": "1b10483bbc38d514"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Cool, let's search through these contents",
   "id": "4bb7c97365d21379"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T02:47:24.587036Z",
     "start_time": "2025-11-09T02:47:24.584605Z"
    }
   },
   "cell_type": "code",
   "source": [
    "content = db.table('content') # can also use db.t.content\n",
    "doc = db.t.docs"
   ],
   "id": "d7ca58f509606121",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T02:47:25.383010Z",
     "start_time": "2025-11-09T02:47:25.380781Z"
    }
   },
   "cell_type": "code",
   "source": "fts_search = bind(content.search, order_by='rank', columns=['id','doc_id', 'content'], limit=50)",
   "id": "3dc042d54229c2fd",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T02:47:26.165703Z",
     "start_time": "2025-11-09T02:47:26.163134Z"
    }
   },
   "cell_type": "code",
   "source": "q = 'economic growth in Europe'",
   "id": "e8aaf0c3f8db4242",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T02:47:26.864948Z",
     "start_time": "2025-11-09T02:47:26.859149Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fts_r = [r for r in fts_search(q)] # FTS search\n",
    "print(len(fts_r), 'results got')"
   ],
   "id": "291ace0fef6d5097",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 results got\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As you can see, there is no exact term `economic growth in Europe` anywhere in the texts. Let's make the search wide. FTS allows globs like * and use OR. There is so much you can tune with apsw fts. We provide strong defaults (always customisable) with the `pre` function.",
   "id": "39cf035081c6d0b7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T02:47:33.049925Z",
     "start_time": "2025-11-09T02:47:33.009189Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fts_r = dict2obj(L(fts_search(pre(q))))\n",
    "print(len(fts_r), 'results got')"
   ],
   "id": "2227c2b6109906a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 results got\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This is just scratching the surface, apsw puts fts on steroids. Look forward for more updates to thedu along these lines. To know more about apsw fts, refer: https://rogerbinns.github.io/apsw/example-fts.html",
   "id": "23760d6d84b5ea54"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Okay, We've got some results. Let's get some vec search results.",
   "id": "b7878bae29f3d1a8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T02:47:37.628123Z",
     "start_time": "2025-11-09T02:47:37.626263Z"
    }
   },
   "cell_type": "code",
   "source": "from chonkie import AutoEmbeddings",
   "id": "97a7b9d74ecf7409",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Chonkie exposes model2vec embeddings which are static embeddings. Static embeddings are 500 times smaller and 50 times faster with a very small performance loss. Potion-retrieval-32M is a good model imo. Learn more at: https://github.com/MinishLab/model2vec",
   "id": "f3a8aeb5f2f9bc13"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T02:47:41.119277Z",
     "start_time": "2025-11-09T02:47:39.498485Z"
    }
   },
   "cell_type": "code",
   "source": "embedding_fn = AutoEmbeddings().get_embeddings('minishlab/potion-retrieval-32M').embed",
   "id": "50f03d675c62a101",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/71293/code/thedu/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T02:47:54.685254Z",
     "start_time": "2025-11-09T02:47:54.640802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vec_r = L(dict2obj(content(select='id, doc_id, content', where='embedding is not null', where_args=dict(qvec=embedding_fn(q).tobytes()), order_by='distance_cosine_f32(embedding, :qvec)',limit=50)))\n",
    "print(len(fts_r), 'results got')"
   ],
   "id": "f37b85c84b7eb97a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 results got\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Cool, we see results. Now, these are ways to get these results separately. The underlying fastlite and apswutils wrappers over sqlite are powerful and allow you to manipulate the db in efficient ways. Checkout: https://github.com/AnswerDotAI/fastlite and https://github.com/AnswerDotAI/apswutils/tree/main",
   "id": "c1e273738bb7a0ec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Thedu provides a search method which reranks the results from both FTS and vector search using Reciprocal Rank Fusion (RRF)\n",
    "> You can always turn it off."
   ],
   "id": "628498d5a7bec5c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T02:49:35.397370Z",
     "start_time": "2025-11-09T02:49:35.319821Z"
    }
   },
   "cell_type": "code",
   "source": "res=db.search(pre(q), embedding_fn(q).tobytes(), columns=['id','doc_id', 'content'], rerank=True)",
   "id": "231ab806887ee8c6",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T02:52:56.800089Z",
     "start_time": "2025-11-09T02:52:56.796849Z"
    }
   },
   "cell_type": "code",
   "source": "[print(r['id'], r['content']) for r in res[:10]]",
   "id": "a445dbcc37aecba5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29855 Economic growth - \n",
      "Creation, growth & market \n",
      "shares of companies having \n",
      "developed innovations in the \n",
      "\n",
      "27089 economic growth and economic cohesion in an inclusive manner, in \n",
      "particular addressing weaknesses of the economy of the Member States, \n",
      "boosting the growth potential of the economy of the Member State \n",
      "concerned, stimulating job creation, and mitigating the adverse \n",
      "effects of the crisis; \n",
      "\n",
      "26418 European relevance structured in six pillars: \n",
      "(a) green transition; \n",
      "(b) digital transformation; \n",
      "(c) smart, sustainable and inclusive growth, including economic \n",
      "cohesion, j\n",
      "29854 Property Rights (IPR) appli­\n",
      "cations \n",
      "Innovations - \n",
      "Number of innovations \n",
      "resulting from the projects \n",
      "funded by the Programme \n",
      "(by type of innovation) \n",
      "including from awarded IPRs \n",
      "Economic growth - \n",
      "Creation, growth & market \n",
      "\n",
      "26649 contribute to strengthening the growth potential, job creation, and \n",
      "economic, social and institutional resilience of the Member State, \n",
      "contributing to the implementation of the European Pillar of Social \n",
      "Rights, including through the promotion of policies for children \n",
      "and the youth, and to mitigating the economic and social impac\n",
      "27088 assessment under this criterion: \n",
      "Scope \n",
      "— the recovery and resilience plan contains measures that aim to foster \n",
      "economic growth and economic cohesion in an inclusive manner, in \n",
      "particular addressing weaknesses of the economy of the Member \n",
      "29595 \n",
      "To address future societal challenges, embrace the opportunities of new tech­\n",
      "nologies and contribute to environmentally friendly and sustainable economic \n",
      "growth, jobs, competitiveness and the well-being of Europe's citizens, there is \n",
      "the need to further strengthen Europe's capacity to innovate by: strengthening \n",
      "\n",
      "26590 (c) a detailed explanation of how the recovery and resilience plan \n",
      "strengthens the growth potential, job creation and economic, \n",
      "\n",
      "27084 represent an adequate response to the economic and social situation of \n",
      "the Member State concerned \n",
      "2.3. The recovery and resilience plan is expected to effectively contribute to \n",
      "strengthening the growth potential, job creation, and economic, social and \n",
      "institutional resilience of the Member State, contributing to the implemen­\n",
      "tation of the European Pillar of Social Rights, including through the \n",
      "\n",
      "29537 sectors, and promoting socio-economic transformations that contribute \n",
      "to inclusion and growth, including migration management and inte­\n",
      "gration of migrants. \n",
      "Areas of intervention: democracy and gov\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "You do not need to, but you can now flashrank these results if needed. The Potion-retrieval model is a bge distilled model which is a fusion cross encoder withs trong retrieval capabilities.",
   "id": "ba65569e0bb66a7b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T02:48:27.112609Z",
     "start_time": "2025-11-09T02:48:27.052759Z"
    }
   },
   "cell_type": "code",
   "source": "from flashrank import Ranker, RerankRequest",
   "id": "64f13c1b75251eb7",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T02:48:28.304199Z",
     "start_time": "2025-11-09T02:48:28.265952Z"
    }
   },
   "cell_type": "code",
   "source": "max_token = int(db.q('select max(tokens) as m from content')[0]['m'])",
   "id": "7858b334f6deac95",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T02:48:31.648350Z",
     "start_time": "2025-11-09T02:48:31.595227Z"
    }
   },
   "cell_type": "code",
   "source": "ranker = Ranker(max_length=max_token+150)",
   "id": "19161c1181a20c0",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T02:49:54.576997Z",
     "start_time": "2025-11-09T02:49:54.536475Z"
    }
   },
   "cell_type": "code",
   "source": "res1=ranker.rerank(RerankRequest(q, res.map(lambda r: dict(text=r.content, id=r.id, meta=dict(doc_id=r.doc_id)))))",
   "id": "7a5e6f6b8292d543",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T02:49:57.767253Z",
     "start_time": "2025-11-09T02:49:57.764118Z"
    }
   },
   "cell_type": "code",
   "source": "[print(r['id'], r['text']) for r in res1[:10]]",
   "id": "12cbd222d681d5b1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29595 \n",
      "To address future societal challenges, embrace the opportunities of new tech­\n",
      "nologies and contribute to environmentally friendly and sustainable economic \n",
      "growth, jobs, competitiveness and the well-being of Europe's citizens, there is \n",
      "the need to further strengthen Europe's capacity to innovate by: strengthening \n",
      "\n",
      "26649 contribute to strengthening the growth potential, job creation, and \n",
      "economic, social and institutional resilience of the Member State, \n",
      "contributing to the implementation of the European Pillar of Social \n",
      "Rights, including through the promotion of policies for children \n",
      "and the youth, and to mitigating the economic and social impac\n",
      "26418 European relevance structured in six pillars: \n",
      "(a) green transition; \n",
      "(b) digital transformation; \n",
      "(c) smart, sustainable and inclusive growth, including economic \n",
      "cohesion, j\n",
      "28129 6.2 Number of enterprises supported by stage (early, growth/expansion) \n",
      "6.3 Number of enterprises supported by Member State and region at NUTS 2 \n",
      "level \n",
      "6.4 Number of enterprises supported by sectors by statistical classification of \n",
      "economic activities in the European Union (NACE) code \n",
      "6.5 Percentage of investment volume under the SME policy window directed \n",
      "towards SMEs \n",
      "7. Social investment and skills \n",
      "\n",
      "27089 economic growth and economic cohesion in an inclusive manner, in \n",
      "particular addressing weaknesses of the economy of the Member States, \n",
      "boosting the growth potential of the economy of the Member State \n",
      "concerned, stimulating job creation, and mitigating the adverse \n",
      "effects of the crisis; \n",
      "\n",
      "21287 used by the European Economic Community (OJ 17, 6.10.1958, p. 385).\n",
      "\n",
      "\n",
      " \n",
      "02018R1726 — EN — 25.04.2024 — 004.001 — 28 \n",
      "\n",
      "27084 represent an adequate response to the economic and social situation of \n",
      "the Member State concerned \n",
      "2.3. The recovery and resilience plan is expected to effectively contribute to \n",
      "strengthening the growth potential, job creation, and economic, social and \n",
      "institutional resilience of the Member State, contributing to the implemen­\n",
      "tation of the European Pillar of Social Rights, including through the \n",
      "\n",
      "29518 Europe, contributing to boosting jobs, growth, and investment, and \n",
      "solving current and future societal challenges. \n",
      "Areas of intervention: nurturing excellence through the mobility of \n",
      "researcher\n",
      "29855 Economic growth - \n",
      "Creation, growth & market \n",
      "shares of companies having \n",
      "developed innovations in the \n",
      "\n",
      "24455 Europe. The evaluations shall examine how the Joint Undertaking fulfils \n",
      "its mission in accordance with its economic, technological, scientific, \n",
      "societal and policy objectives, including climate-relat\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, let's compare flashrank results and thedu rerank results.",
   "id": "567a27350f67974f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T02:51:33.604238Z",
     "start_time": "2025-11-09T02:51:33.599987Z"
    }
   },
   "cell_type": "code",
   "source": "[print(r1['id'], r2['id']) for r1,r2 in zip(res,res1)]",
   "id": "114e7e08e4892ffb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29855 29595\n",
      "27089 26649\n",
      "26418 26418\n",
      "29854 28129\n",
      "26649 27089\n",
      "27088 21287\n",
      "29595 27084\n",
      "26590 29518\n",
      "27084 29855\n",
      "29537 24455\n",
      "26591 28150\n",
      "26648 26425\n",
      "26428 29517\n",
      "29538 26006\n",
      "26419 17580\n",
      "28129 26428\n",
      "29850 24862\n",
      "26425 27973\n",
      "29518 28399\n",
      "26424 29127\n",
      "29068 28716\n",
      "26006 22493\n",
      "27430 24473\n",
      "24862 26419\n",
      "29517 26591\n",
      "26993 26570\n",
      "27076 29537\n",
      "27973 24860\n",
      "29852 24868\n",
      "17580 29854\n",
      "22493 25783\n",
      "28160 29068\n",
      "29732 27076\n",
      "29470 24159\n",
      "11996 27088\n",
      "28399 26993\n",
      "30544 29850\n",
      "26014 27444\n",
      "28150 14899\n",
      "28145 30544\n",
      "29127 29470\n",
      "28341 11997\n",
      "29469 2557\n",
      "21287 26590\n",
      "28161 29469\n",
      "29733 26424\n",
      "24473 29538\n",
      "28716 11996\n",
      "11997 26648\n",
      "24455 28145\n",
      "26570 3194\n",
      "14899 25728\n",
      "25728 26014\n",
      "27406 25531\n",
      "28992 28149\n",
      "3194 27406\n",
      "25531 27430\n",
      "24159 28161\n",
      "24868 29733\n",
      "28149 24542\n",
      "24566 28992\n",
      "24863 27030\n",
      "24860 29732\n",
      "27030 28341\n",
      "2557 24566\n",
      "31356 24220\n",
      "24542 24863\n",
      "28448 28160\n",
      "33126 16562\n",
      "12052 31356\n",
      "16562 33126\n",
      "11463 29852\n",
      "25783 28448\n",
      "33125 12052\n",
      "24220 11463\n",
      "27444 33125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "So you have it. a simple rag pipeline.",
   "id": "a8028b71e4f94f08"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
