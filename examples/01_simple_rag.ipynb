{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Simple RAG with Thedu\n",
    "> We will build a simple rag with thedu. Do not be deceived. We are doing a whole bunch of heavylifting under the hood with very little code."
   ],
   "id": "94f1234b99572d4e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T04:16:38.928596Z",
     "start_time": "2025-11-06T04:16:38.632718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "from thedu import *\n",
    "from fastcore.all import *\n",
    "import re\n",
    "from selectolax.parser import HTMLParser"
   ],
   "id": "3edd79a6c9ca938",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Ingest a PDF document\n",
    "> We will ingest a sample PDF document from Bruegel.\n",
    "> We will read the PDF document using `read_pdf` function from thedu.ingest module.\n",
    "> We will then scrape the urls from the pdf and then recursively get all pdf's off of those links and ingest them as well.'"
   ],
   "id": "bbe74532d19fe55b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T04:16:43.832265Z",
     "start_time": "2025-11-06T04:16:43.829783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@patch\n",
    "def mk_dest(self:Path, add='_1', suffix=None, force=False):\n",
    "    \"\"\"Add a suffix to the file name before the extension.\"\"\"\n",
    "    if not self.exists(): return self\n",
    "    self=self.parent/self.stem + add + ifnone(suffix, self.suffix)\n",
    "    return self.mk_dest(add, suffix, force) if self.exists() and force else self"
   ],
   "id": "c183600c29b58e78",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T04:16:44.523427Z",
     "start_time": "2025-11-06T04:16:44.516991Z"
    }
   },
   "cell_type": "code",
   "source": [
    "p= '/Users/71293/code/thedu/examples/bruegel_dataset/links/?uri=CELEX%3A02017R2394-20220101_1.html'\n",
    "meta_nodes = HTMLParser(Path(p).read_text()).tags('meta')\n",
    "n_all = L([dict2obj(m.attributes) for m in meta_nodes]).filter(lambda m: 'about' in m)\n",
    "n = n_all.filter(lambda m: ('property' in m and m['property'].lower() in ['eli:is_embodied_by','eli:title'])\n",
    ").filter(lambda m: ('resource' in m and 'pdfa2a' in m['resource'].lower()) or 'content' in m).groupby('about')"
   ],
   "id": "f4401d663d9d8e24",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T04:16:48.932554Z",
     "start_time": "2025-11-06T04:16:48.930839Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nm=dict()\n",
    "for k in n: nm[k] = merge(*n[k])"
   ],
   "id": "fcdff9f1ddf9ed12",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T05:25:45.924330Z",
     "start_time": "2025-11-06T05:25:45.917988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BruegelDataset:\n",
    "    ''' Dataset for Bruegel PDF documents.'''\n",
    "    URL = 'https://www.bruegel.org/system/files/2024-06/Bruegel_factsheet_2024_0.pdf'\n",
    "    URI_SCHEMA = r'^http://data\\.europa\\.eu/eli/(?P<typedoc>[^/]+)/(?P<year>\\d{4})/(?P<natural_number>\\d+)/(?P<date>\\d{4}-\\d{2}-\\d{2})/(?P<lang>[a-z]{2,3})/pdfa2a$'\n",
    "    def __init__(self, dest:Path=Path('bruegel_dataset')):\n",
    "        self.dest = dest\n",
    "        self.pdfs = self()\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _is_pdf_link(l:str): return l.strip() and (l.lower().endswith('.pdf') or 'pdf' in l.lower())\n",
    "    @staticmethod\n",
    "    def url2name(url: str) -> str | None:\n",
    "        if re.match(BruegelDataset.URI_SCHEMA, url):\n",
    "            m = re.match(BruegelDataset.URI_SCHEMA, url)\n",
    "            return f\"{m['typedoc']}_{m['year']}_{m['natural_number']}_{m['date']}_{m['lang']}.pdf\"\n",
    "        return url.rstrip('/').split('/')[-1]\n",
    "\n",
    "    def _get_meta(self, r):\n",
    "        try:\n",
    "            meta = HTMLParser(r).tags('meta')\n",
    "            nodes = L([dict2obj(m.attributes) for m in meta]).filter(\n",
    "                lambda m: 'about' in m\n",
    "            ).filter(\n",
    "                lambda m: ('property' in m and m['property'].lower() in ['eli:is_embodied_by','eli:title'])\n",
    "            ).filter(\n",
    "                lambda m: ('resource' in m and 'pdfa2a' in m['resource'].lower()) or 'content' in m\n",
    "            ).groupby('about')\n",
    "            for k in nodes: nodes[k] = merge(*nodes[k])\n",
    "            return L([(nodes[n]['resource'], nodes[n]['content']) for n in nodes])\n",
    "        except: pass\n",
    "\n",
    "\n",
    "    def read_link(self, l, dest=None):\n",
    "        try: return self.save_pdf(l, dest).read_text()\n",
    "        except: return ''\n",
    "\n",
    "    def save_pdf(self, l:str, dest:Path=None) -> Path | None:\n",
    "        try:\n",
    "            if not l : return None\n",
    "            if not dest: dest = self.dest\n",
    "            p = dest / self.url2name(l)\n",
    "            if not p.exists(): p = urlsave(l,p)\n",
    "            return p\n",
    "        except Exception as ex: print(ex); return None\n",
    "\n",
    "    def __call__(self) -> L:\n",
    "        '''Make a dataset of documents from a pdf url and all linked pdfs.'''\n",
    "\n",
    "        def get_linked_pdfs(doc, filter=True):\n",
    "            links = doc.map(lambda p: p.links.map(lambda l: l.uri)).concat().unique()\n",
    "            if filter: links = links.filter(self._is_pdf_link)\n",
    "            return links\n",
    "\n",
    "        pth = self.dest / self.url2name(self.URL)\n",
    "        if not pth.exists: pth = urlsave(self.URL, self.dest / self.url2name(self.URL))\n",
    "        main_doc = read_pdf(pth)\n",
    "        pdf_lp = Path(self.dest / 'pdf_links')\n",
    "        if not pdf_lp.exists():\n",
    "            links = get_linked_pdfs(main_doc, filter=False)\n",
    "            print(f'Found {len(links)} linked pdfs.')\n",
    "            all_c = parallel(self.read_link, links, threadpool=True, dest=self.dest/'links')\n",
    "            pdf_links = parallel(self._get_meta, all_c).concat().unique()\n",
    "            print(f'Found {len(pdf_links)} external pdf links.')\n",
    "        url2tit = {l.split(',')[0]: l.split(',')[1] for l in pdf_lp.readlines()}\n",
    "        pdf_list = L([l.split(',')[0] for l in pdf_lp.readlines()])\n",
    "        name2url = {self.url2name(l): l for l in pdf_list}\n",
    "        pdf_set = set(pdf_list.map(self.url2name))\n",
    "        downloaded_pdfs = globtastic(self.dest / 'pdfs', file_glob='*.pdf', func=Path).map(lambda p: p.name)\n",
    "        fetch_list = [name2url.get(d) for d in pdf_set.difference(downloaded_pdfs)]\n",
    "        if fetch_list:\n",
    "            print(f'Downloading {len(fetch_list)} new pdfs...')\n",
    "            parallel(self.save_pdf, fetch_list, dest=self.dest/'pdfs')\n",
    "        return globtastic(self.dest / 'pdfs', file_glob='*.pdf', func=Path).map(lambda m: AttrDict(path=m, title=url2tit[name2url[m.name]]))"
   ],
   "id": "fe6fbcae5619ee36",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T05:25:48.863391Z",
     "start_time": "2025-11-06T05:25:48.551786Z"
    }
   },
   "cell_type": "code",
   "source": "B=BruegelDataset()",
   "id": "efb14b2e333b565d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 1 new pdfs...\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T05:42:50.915109Z",
     "start_time": "2025-11-06T05:42:50.912623Z"
    }
   },
   "cell_type": "code",
   "source": "from chonkie import RecursiveChunker",
   "id": "fc870ad5bf172bd9",
   "outputs": [],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T05:43:05.113997Z",
     "start_time": "2025-11-06T05:42:53.469435Z"
    }
   },
   "cell_type": "code",
   "source": "chunker = RecursiveChunker()",
   "id": "a41fcdf86b2fe8b4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2025-11-06 16:43:02.256\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36mchonkie.chunker.base\u001B[0m:\u001B[36m__init__\u001B[0m:\u001B[36m32\u001B[0m - \u001B[34m\u001B[1mInitialized RecursiveChunker\u001B[0m\n",
      "/Users/71293/code/thedu/.venv/lib/python3.13/site-packages/chonkie/embeddings/auto.py:87: UserWarning: Failed to load minishlab/potion-base-32M with Model2VecEmbeddings: model2vec is not available. Please install it via `pip install chonkie[model2vec]`\n",
      "Falling back to loading default provider model.\n",
      "  warnings.warn(\n",
      "/Users/71293/code/thedu/.venv/lib/python3.13/site-packages/chonkie/embeddings/auto.py:95: UserWarning: Failed to load the default model for Model2VecEmbeddings: model2vec is not available. Please install it via `pip install chonkie[model2vec]`\n",
      "Falling back to SentenceTransformerEmbeddings.\n",
      "  warnings.warn(\n",
      "\u001B[32m2025-11-06 16:43:05.112\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36mchonkie.chunker.base\u001B[0m:\u001B[36m__init__\u001B[0m:\u001B[36m32\u001B[0m - \u001B[34m\u001B[1mInitialized SemanticChunker\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2025-11-06 16:39:41.905\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36mchonkie.chunker.base\u001B[0m:\u001B[36m__init__\u001B[0m:\u001B[36m32\u001B[0m - \u001B[34m\u001B[1mInitialized RecursiveChunker\u001B[0m\n",
      "\u001B[32m2025-11-06 16:39:42.019\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36mchonkie.chunker.recursive\u001B[0m:\u001B[36mchunk\u001B[0m:\u001B[36m368\u001B[0m - \u001B[34m\u001B[1mStarting recursive chunking for text of length 17277\u001B[0m\n",
      "\u001B[32m2025-11-06 16:39:42.020\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36mchonkie.chunker.recursive\u001B[0m:\u001B[36mchunk\u001B[0m:\u001B[36m370\u001B[0m - \u001B[1mCreated 9 chunks using recursive chunking\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 74,
   "source": "chunks = chunker('\\n'.join(read_pdf(B.pdfs[0].path).map(lambda p: p.text_plain)))",
   "id": "e16f4a5619fb798c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T05:39:58.475553Z",
     "start_time": "2025-11-06T05:39:58.472950Z"
    }
   },
   "cell_type": "code",
   "source": "for c in chunks[:3]: print(c[:500], '\\n---\\n')",
   "id": "b38816e8fe9fe83a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Този текст служи само за информационни цели и няма правно действие. Институциите на Съюза не носят \n",
      "отговорност за неговото съдържание. Автентичните версии на съответните актове, включително техните преамбюли, \n",
      "са версиите, публикувани в Официален вестник на Европейския съюз и налични в EUR-Lex. Тези официални \n",
      "текстове са пряко достъпни чрез връзките, публикувани в настоящия документ \n",
      "►B \n",
      "ДИРЕКТИВА 93/13/ЕИО НА СЪВЕТА \n",
      "от 5 април 1993 година \n",
      "относно неравноправните клаузи в потребителските дог \n",
      "---\n",
      "\n",
      "в) „продавач или доставчик“ означава всяко физическо или \n",
      "юридическо лице, което в качеството си на страна по дого­ \n",
      "ворите, предмет на настоящата директива, участва поради \n",
      "интереси, които са свързани със занятието, стопанската \n",
      "дейност или професията му, независимо дали в публичноправен \n",
      "или частноправен контекст. \n",
      "Член 3 \n",
      "1. \n",
      "В случаите, когато дадена договорна клауза не е индиви­ \n",
      "дуално договорена, се счита за неравноправна, когато въпреки \n",
      "изискването за добросъвестност, тя създава в ущърб \n",
      "---\n",
      "\n",
      "услугите, които ще се предоставят в замяна, от друга страна, при \n",
      "условие че тези клаузи са изразени на ясен и разбираем език. \n",
      "▼B \n",
      "Член 5 \n",
      "При договори, в които всички или определени клаузи се предлагат \n",
      "на потребителя в писмен вид, тези условия се съставят на ясен и \n",
      "разбираем език. При наличие на съмнение за смисъла на \n",
      "определена клауза, тя се тълкува в най-благоприятен за потре­ \n",
      "бителя смисъл. Настоящото правило не се прилага във връзка с \n",
      "процедурите по реда на член 7, параграф 2. \n",
      "Член 6 \n",
      "---\n",
      "\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "31c818f42ceb33bc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
