{
 "cells": [
  {
   "cell_type": "raw",
   "id": "962855a7b0c50ee",
   "metadata": {},
   "source": [
    "---\n",
    "description: encoding utilities for onnx based text encoders\n",
    "output-file: index.html\n",
    "title: utils\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20814b2380dccb5",
   "metadata": {},
   "outputs": [],
   "source": "#| default_exp utils"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a23f4f46a4aed64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T00:25:16.267683073Z",
     "start_time": "2025-12-16T00:25:16.151154274Z"
    }
   },
   "source": [
    "#| export\n",
    "from fastcore.all import *\n",
    "import json\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import os\n",
    "from tokenizers import Tokenizer, AddedToken"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "4fce0767837bdcb1",
   "metadata": {},
   "source": "Let's load some default models that work well off the box for various tasks"
  },
  {
   "cell_type": "code",
   "id": "5b8ed63b321a17d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T00:25:17.886599139Z",
     "start_time": "2025-12-16T00:25:17.876634604Z"
    }
   },
   "source": [
    "#| export\n",
    "embedding_gemma_prompt = AttrDict(\n",
    "    document='Instruct: document \\n document: {text}',\n",
    "    query='Instruct: query \\n query: {text}',\n",
    ")\n",
    "modernbert_prompt = AttrDict(\n",
    "    document='search_document: {text}',\n",
    "    query='search_query: {text}',\n",
    ")\n",
    "embedding_gemma = AttrDict(model='onnx-community/embeddinggemma-300m-ONNX', onnx_path='onnx/model.onnx', prompt=embedding_gemma_prompt)\n",
    "modernbert = AttrDict(model='nomic-ai/modernbert-embed-base', onnx_path='onnx/model.onnx', prompt=modernbert_prompt)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "a1db0a2e242d9f67",
   "metadata": {},
   "source": "FastEncode is an onnx based embedding model wrapper that can work with most onnx model with a huggingface tokenizer. (The Qwen models are a bit tricky due to their padding token handling so they need a custom wrapper which we will add later)"
  },
  {
   "cell_type": "code",
   "id": "fe4371ef8e2b9b52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T00:27:06.989721157Z",
     "start_time": "2025-12-16T00:27:06.970348136Z"
    }
   },
   "source": [
    "#| export\n",
    "class FastEncode:\n",
    "\tdef __init__(self,\n",
    "\t\t\t\t model_dict=embedding_gemma,# model dict with model repo, onnx path and prompt templates\n",
    "\t\t\t\t repo_id=None,              # model repo on HF. needs to have onnx model file\n",
    "\t\t\t\t md=None,                   # local model dir\n",
    "\t\t\t\t md_nm=None,                # onnx model file name\n",
    "\t\t\t\t normalize=True,            # normalize embeddings\n",
    "\t\t\t\t dtype=np.float16,          # output dtype\n",
    "\t\t\t\t tti=False,                 # use token type ids\n",
    "\t\t\t\t prompt=None,               # prompt templates\n",
    "\t\t\t\t hf_token=None              # HF token. you can also set HF_TOKEN env variable\n",
    "\t):\n",
    "\t\t'''Fast ONNX-based text encoder'''\n",
    "\t\tassert (model_dict is None) != (repo_id is None), 'Either model_dict or repo_id must be provided and not both'\n",
    "\t\trepo_id = model_dict.model if model_dict else repo_id\n",
    "\t\tmd = md or (model_dict.model if model_dict else repo_id)\n",
    "\t\tmd_nm = md_nm or (model_dict.onnx_path if model_dict else 'onnx/model.onnx')\n",
    "\t\tprompt = prompt or (model_dict.prompt if model_dict else AttrDictDefault())\n",
    "\t\tstore_attr()\n",
    "\t\ttry: self.md = download_model(repo_id=repo_id, md=md, token=hf_token)\n",
    "\t\texcept Exception as ex: print(f'model download failed: {ex}. hint: is hf_token set')\n",
    "\t\tself._load_enc()\n",
    "\tdef _load_enc(self):\n",
    "\t\ttry:\n",
    "\t\t\tonnx_p = Path(self.md)/ self.md_nm\n",
    "\t\t\tsess_opt = ort.SessionOptions()\n",
    "\t\t\tsess_opt.intra_op_num_threads = os.cpu_count() or 1\n",
    "\t\t\tsess_opt.execution_mode = ort.ExecutionMode.ORT_PARALLEL\n",
    "\t\t\tsess_opt.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "\t\t\tself._load_tok()\n",
    "\t\t\txtra=filter_ex(ort.get_available_providers(),lambda x: x in ('CUDAExecutionProvider','CoreMLExecutionProvider'))\n",
    "\t\t\ttry: self.sess = ort.InferenceSession(onnx_p, sess_opt, providers=xtra+[\"CPUExecutionProvider\"])\n",
    "\t\t\texcept Exception as ex: self.sess = ort.InferenceSession(onnx_p, sess_opt, providers=[\"CPUExecutionProvider\"])\n",
    "\t\texcept Exception as ex:\n",
    "\t\t\tprint(f'Encoding setup errored out with exception: {ex}')\n",
    "\t\t\tself.sess = None\n",
    "\tdef _load_tok(self):\n",
    "\t\tcfg = json.load(open(os.path.join(self.md, \"config.json\")))\n",
    "\t\ttok_cfg = json.load(open(os.path.join(self.md, \"tokenizer_config.json\")))\n",
    "\t\ttok_map = json.load(open(os.path.join(self.md, \"special_tokens_map.json\")))\n",
    "\t\tself.tok = Tokenizer.from_file(os.path.join(self.md, \"tokenizer.json\"))\n",
    "\t\tself.tok.enable_padding(pad_id=cfg[\"pad_token_id\"], pad_token=tok_cfg[\"pad_token\"])\n",
    "\t\tself.tok.enable_truncation(max_length=min(tok_cfg['model_max_length'], 512))\n",
    "\t\tfor t in tok_map.values(): self.tok.add_special_tokens(\n",
    "\t\t\t[t if isinstance(t, str) else AddedToken(**t) if isinstance(t, dict) else None])\n",
    "\tdef _enc(self, txts:list, dtype=np.int64):\n",
    "\t\tencs = self.tok.encode_batch(txts, add_special_tokens=True)\n",
    "\t\tids = np.array([e.ids for e in encs], dtype=dtype)\n",
    "\t\tmsk = np.array([e.attention_mask for e in encs], dtype=dtype)\n",
    "\t\treturn ids, msk\n",
    "\tdef _mp(self, mout: np.ndarray, msk: np.ndarray):\n",
    "\t\ttoken_embeddings = mout\n",
    "\t\tinput_mask_expanded = np.expand_dims(msk, axis=-1)\n",
    "\t\tinput_mask_expanded = np.broadcast_to(input_mask_expanded, token_embeddings.shape)\n",
    "\t\tsum_embeddings = np.sum(token_embeddings * input_mask_expanded, axis=1)\n",
    "\t\tsum_mask = np.clip(np.sum(input_mask_expanded, axis=1), a_min=1e-9, a_max=None)\n",
    "\t\treturn sum_embeddings / sum_mask\n",
    "\tdef encode(self, lns:list, **kw):\n",
    "\t\tif not self.sess: print('ONNX session not initialized properly. Fix error during initialisation'); return None\n",
    "\t\tif not lns: return np.zeros((0, self.sess.get_outputs()[0].shape[-1]), dtype=self.dtype)\n",
    "\t\tids, msk = self._enc(lns)\n",
    "\t\tif ids.ndim ==1: ids, msk = np.expand_dims(ids, axis=0), np.expand_dims(msk, axis=0)\n",
    "\t\tinp = dict(input_ids=ids,attention_mask=msk)\n",
    "\t\tif self.tti: inp['token_type_ids']=np.zeros(ids.shape, dtype=np.int64)\n",
    "\t\to=self._mp(self.sess.run(None, inp)[0], msk)\n",
    "\t\tif self.normalize: o = o / np.clip(np.linalg.norm(o, ord=2, axis=1, keepdims=True), 1e-12, None)\n",
    "\t\treturn o.astype(self.dtype)\n",
    "\tdef encode_document(self, lns, prompt:str=None, **kw):\n",
    "\t\tif prompt is None: prompt = self.prompt.get('document', None)\n",
    "\t\treturn self.encode(L(lns).map(lambda l: prompt.format(text=l) if prompt else l), **kw)\n",
    "\tdef encode_query(self, lns, prompt:str=None, **kw):\n",
    "\t\tif prompt is None: prompt = self.prompt.get('query', None)\n",
    "\t\treturn self.encode(L(lns).map(lambda l: prompt.format(text=l) if prompt else l), **kw)\n",
    "\n",
    "def download_model(repo_id=embedding_gemma.model,   # model repo on HF\n",
    "\t\t\t   md=embedding_gemma.model,        # local model dir\n",
    "\t\t\t   token=None                       # HF token. you can also set HF_TOKEN env variable\n",
    "):\n",
    "\t'''Download model from HF hub'''\n",
    "\tif Path(md).exists(): return md\n",
    "\timport huggingface_hub as hf\n",
    "\treturn hf.snapshot_download(repo_id=repo_id, local_dir=md, token=token or os.getenv('HF_TOKEN'))"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "6714685e60c2c7f6",
   "metadata": {},
   "source": "Let's quickly check if the encoder is working"
  },
  {
   "cell_type": "code",
   "id": "16d5063e59c2511",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T00:27:09.680854451Z",
     "start_time": "2025-12-16T00:27:08.375275231Z"
    }
   },
   "source": "enc=FastEncode()",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[0;93m2025-12-16 11:27:09.334066665 [W:onnxruntime:, transformer_memcpy.cc:111 ApplyImpl] 736 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001B[m\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T00:27:11.463237828Z",
     "start_time": "2025-12-16T00:27:11.413342808Z"
    }
   },
   "cell_type": "code",
   "source": "enc.encode_document(['This is a test', 'Another test'])",
   "id": "20eb94077fa64b48",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.05774 ,  0.001704,  0.002562, ..., -0.06177 , -0.00661 ,\n",
       "         0.03174 ],\n",
       "       [ 0.02939 , -0.008194, -0.00918 , ..., -0.02846 , -0.002222,\n",
       "         0.02847 ]], shape=(2, 768), dtype=float16)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "c5dfc1b1b1c0a41c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T00:27:13.420466820Z",
     "start_time": "2025-12-16T00:27:12.731279985Z"
    }
   },
   "source": "modern_enc=FastEncode(modernbert)",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T00:27:14.397859802Z",
     "start_time": "2025-12-16T00:27:14.364086329Z"
    }
   },
   "cell_type": "code",
   "source": "modern_enc.encode_query(['This is a test', 'Another test'])",
   "id": "62429df2872580ae",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.05026 , -0.04352 , -0.0171  , ..., -0.04974 ,  0.01598 ,\n",
       "        -0.07056 ],\n",
       "       [-0.05093 , -0.02133 , -0.0368  , ..., -0.10736 , -0.000944,\n",
       "        -0.01177 ]], shape=(2, 768), dtype=float16)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3732bacefdb81cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
