{
 "cells": [
  {
   "cell_type": "raw",
   "id": "dca3da88c5c8a1e7",
   "metadata": {},
   "source": [
    "---\n",
    "description: Building blocks for ingesting and querying data with thedu with some\n",
    "  additional utils\n",
    "output-file: index.html\n",
    "title: data\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd2cae8a5079bb4",
   "metadata": {},
   "source": [
    "We will build a simple ingestion pipeline to ingest pdf documents into litesearch database for searching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58f453150128627",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1e955269e0d234",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from chonkie import Pipeline, Chunk\n",
    "from fastcore.all import L, concat, patch, ifnone\n",
    "from fastlite import Database\n",
    "import os\n",
    "import pymupdf\n",
    "from pymupdf import Document, Pixmap, csRGB, Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc014ece1162af13",
   "metadata": {},
   "outputs": [],
   "source": "#| export\n@patch\ndef get_texts(self: Document, st=0, end=-1, **kw):\n\treturn L(self[st:end]).map(lambda p: p.get_text(**kw))\n\n@patch\ndef get_links(self: Document, st=0, end=-1):\n\treturn L(self[st:end]).map(lambda p: p.get_links()).concat()\n\n@patch\ndef ext_im(self: Document, it=None):\n    if not it: return None\n    assert isinstance(it, tuple) and len(it) > 2, 'Invalid image tuple'\n    xref, smask = it[0], it[1]\n    if smask > 0:\n        pix0 = Pixmap(self.extract_image(xref)['image'])\n        if pix0.alpha: pix0 = Pixmap(pix0, 0)  # remove alpha channel\n        mask = Pixmap(self.extract_image(smask)['image'])\n        try: pix = Pixmap(pix0, mask)\n        except (RuntimeError, ValueError, KeyError): pix = Pixmap(self.extract_image(xref)['image'])\n        ext = 'pam' if pix0.n > 3 else 'png'\n        return dict(ext=ext, colorspace=pix.colorspace.n, image=pix.tobytes(ext))\n    if '/ColorSpace' in self.xref_object(xref, compressed=True):\n        pix = Pixmap(csRGB, Pixmap(self, xref))\n        return dict(ext='png', colorspace=3, image=pix.tobytes('png'))\n    return self.extract_image(xref)\n\n@patch\ndef ext_imgs(self: Document, st=0, end=-1):\n\tf = lambda p: [ext_im(self,it) for it in p.get_images(full=True)]\n\treturn L(self[st:end]).map(f).concat()"
  },
  {
   "cell_type": "markdown",
   "id": "f3d011bd4cd59886",
   "metadata": {},
   "source": [
    "\n",
    "#### Some utilities for pdf processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa2d46fdfc3b93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def text_pipe():\n",
    "    'Return the default chunking and embedding pipeline for texts.'\n",
    "    return (Pipeline()\n",
    "            .chunk_with('recursive', tokenizer='gpt2', chunk_size=2048)\n",
    "            .chunk_with('semantic', chunk_size=1024)\n",
    "            .refine_with('overlap', context_size=128)\n",
    "            .refine_with('embeddings', embedding_model='minishlab/potion-retrieval-32M'))\n",
    "\n",
    "def chunk_fn(f=None):\n",
    "\t'Return the chunking function. If None, use default chunk fn'\n",
    "\treturn ifnone(f, text_pipe().run)\n",
    "\n",
    "def chunk(lns, f=None): return chunk_fn(f)(lns).chunks\n",
    "\n",
    "@patch\n",
    "def pg2chunks(\n",
    "\t\tself:Page,      # pdf path\n",
    "        fn=None   # chunking fn. If None, use default chunk fn\n",
    "):\n",
    "    'Return a list of text chunks for a document.'\n",
    "    return chunk(self.get_text(), fn)\n",
    "\n",
    "@patch()\n",
    "def content(self:Chunk, xtra:dict=None):\n",
    "\tmeta = dict(tokens=self.token_count, start_index=self.start_index,\n",
    "\t            end_index=self.end_index, context=self.context, **(xtra or dict()))\n",
    "\treturn dict(content=self.text, embedding=self.embedding.tobytes(), metadata=meta)\n",
    "\n",
    "@patch\n",
    "def to_content(self:Document, fn=None):\n",
    "\t'''Return chunks of text for a document.'''\n",
    "\tm = dict(doc_name=self.metadata['title'], doc_page_count=self.page_count,doc_metadata=self.metadata, doc_toc=self.get_toc())\n",
    "\treturn L(self.pages()).renumerate().map(\n",
    "\t\tlambda p: L(p[0].pg2chunks(fn)).map(\n",
    "\t\t\tlambda c: c.content(dict(pg_no=p[1]+1, **m)))).concat()\n",
    "@patch\n",
    "def pdf_ingest(\n",
    "    self: Database,        # litesearch database connection\n",
    "    path: os.PathLike,     # pdf path\n",
    "    fn=None,         # chunking function. If None, use default chunk fn\n",
    "    tbl: str = 'content',  # content table name\n",
    "):\n",
    "    'Ingest PDF documents into litesearch.'\n",
    "    self.mk_store(tbl).insert_all(pymupdf.open(path).to_content(fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f887a2d1e48c7e1d",
   "metadata": {},
   "outputs": [],
   "source": "#| export\ndef clean(q:str  # query to be passed for fts search\n          ) -> str:\n    '''Clean the query by removing * and returning empty string for empty queries.'''\n    if not q or not q.strip():\n        return ''\n    return q.replace('*', '')\n\ndef add_wc(q:str  # query to be passed for fts search\n           ) -> str:\n    '''Add wild card * to each word in the query.'''\n    if not q or not q.strip():\n        return ''\n    return ' '.join(map(lambda w: w + '*', q.split(' ')))\n\ndef mk_wider(q:str  # query to be passed for fts search\n             ) -> str:\n    '''Widen the query by joining words with OR operator.'''\n    if not q or not q.strip():\n        return ''\n    return ' OR '.join(map(lambda w: f'{w}', q.split(' ')))\n\ndef kw(q:str  # query to be passed for fts search\n       ) -> str:\n    '''Extract keywords from the query using YAKE library.'''\n    from yake import KeywordExtractor as KW\n    return ' '.join((set(concat([k.split(' ') for k, s in KW().extract_keywords(q)]))))\n\ndef pre(q:str,          # query to be passed for fts search\n        wc=True,        # add wild card to each word\n        wide=True,      # widen the query with OR operator\n        extract_kw=True # extract keywords from the query\n        ) -> str:\n    '''Preprocess the query for fts search.'''\n    q = clean(q)\n    if not q:\n        return ''\n    if extract_kw: q = kw(q)\n    if wc: q = add_wc(q)\n    if wide: q = mk_wider(q)\n    return q"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf6852b35d08be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae77ab3a5f02bfa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}