{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Fast hybrid search (text + vectors), automatically reranked.\n",
    "output-file: index.html\n",
    "title: litesearch\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NB** If you're reading this in GitHub readme, I recommend you read the more nicely formatted [documentation format][docs] of this tutorial.\n",
    "\n",
    "[docs]: https://Karthik777.github.io/litesearch/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Litesearch is a lightweight library to set up a [fastlite][fl] database with FTS5 and vector search capabilities using [usearch][unum].\n",
    "\n",
    "Litesearch uses usearch sqlite extensions to provide fast vector search capabilities and combines it  with sqlite's FTS5 capabilities to provide hybrid search.\n",
    "- Litesearch uses fastlite, which is a lightweight wrapper around SQLite that makes SQLite database management delightful. It uses apsw rather than sqlite3 and provides best practices OOB.\n",
    "- Usearch is a cross-language package which provides vector search capabilities. We're using its sqlite extensions here to provide fast vector search capabilities.\n",
    "\n",
    "Lite search provides a simple way to setup this database using the `database()` method. You get a store with FTS5 and vector search capabilities using the `get_store()` method and you can search through the contents using the `search()` method.\n",
    "\n",
    "Litesearch also provides document and code manipulation tools as part of the `data` module and onnx based text encoders as part of the `utils` module.\n",
    "- litesearch extends pymupdf Document and Page classes to extract texts, images and links easily.\n",
    "- litesearch provides onnx based text encoders which can be used to generate embeddings for documents and queries.\n",
    "- litesearch provides a quick code parsing utility to parse python files into code chunks for ingestion.\n",
    "\n",
    "\n",
    "[fl]: https://fastlite.answer.ai/\n",
    "[unum]: https://unum-cloud.github.io/USearch/sqlite/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Get Started"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "fastlite and usearch will be installed automatically with litesearch if you do not have it already."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install litesearch -qq"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Litesearch only adds dependencies it needs, so you can use import * from litesearch without worrying about heavy dependencies.\n",
    "> First time import will try to setup usearch extensions and installing libsqlite3 if you do not have it already. mac also needs an extra step to add libsqlite3 into it's `LC_PATH`. Check `postfix.py` for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from litesearch import *"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### database"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sqlite_version': '3.51.1'}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = database()\n",
    "db.q('select sqlite_version() as sqlite_version')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Let's try some of usearch's distance functions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'sqeuclidean_v1_v2': 100.0, 'sqeuclidean_v1_v3': 56.25, 'sqeuclidean_v2_v3': 6.25}]\n",
      "[{'divergence_v1_v2': 34.657352447509766, 'divergence_v1_v3': 12.046551704406738, 'divergence_v2_v3': 8.66433334350586}]\n",
      "[{'inner_v1_v2': 1.0, 'inner_v1_v3': -24.0, 'inner_v2_v3': 1.0}]\n",
      "[{'cosine_v1_v2': 1.0, 'cosine_v1_v3': 0.0, 'cosine_v2_v3': 1.0}]\n"
     ]
    }
   ],
   "source": [
    "embs = dict(\n",
    "\tv1=np.ones((100,),dtype=np.float32).tobytes(), \t\t# vector of ones\n",
    "\tv2=np.zeros((100,),dtype=np.float32).tobytes(), \t# vector of zeros\n",
    "\tv3=np.full((100,),0.25,dtype=np.float32).tobytes() \t# vector of 0.25s\n",
    ")\n",
    "def dist_q(metric):\n",
    "\treturn db.q(f'''\n",
    "\t\tselect\n",
    "\t\t\tdistance_{metric}_f32(:v1,:v2) as {metric}_v1_v2,\n",
    "\t\t\tdistance_{metric}_f32(:v1,:v3) as {metric}_v1_v3,\n",
    "\t\t\tdistance_{metric}_f32(:v2,:v3) as {metric}_v2_v3\n",
    "\t''', embs)\n",
    "\n",
    "for fn in ['sqeuclidean', 'divergence', 'inner', 'cosine']: print(dist_q(fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### store\n",
    "> A store is a table with FTS5 and vector search capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CREATE TABLE [store] (\\n   [content] TEXT NOT NULL,\\n   [embedding] BLOB,\\n   [metadata] TEXT,\\n   [uploaded_at] FLOAT DEFAULT CURRENT_TIMESTAMP,\\n   [id] INTEGER PRIMARY KEY\\n)'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store = db.get_store()\n",
    "store.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a naive embedder for testing.\n",
    "> Checkout `FastEncode` in `utils` module for onnx based text encoders.\n",
    "> Check the `examples` folder for usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.feature_extraction.text import TfidfVectorizer"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.   , 0.   , 0.   , 0.   , 0.   , 1.   ],\n",
       "       [1.   , 0.   , 0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.   , 0.577, 0.577, 0.   , 0.577, 0.   ],\n",
       "       [0.619, 0.   , 0.   , 0.785, 0.   , 0.   ]], dtype=float16)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txts, q = ['this is a text', \"I'm hungry\", \"Let's play! shall we?\"], 'playing hungry'\n",
    "# this is naive vectoriser intended to showcase litesearch. In practice, use a proper text encoder.\n",
    "def embed_texts(texts): return TfidfVectorizer(max_features=20000, stop_words='english').fit_transform(texts).toarray().astype(np.float16)\n",
    "embs = embed_texts(txts + [q])  # last one is query\n",
    "embs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "> usearch also works with json embeddings, but using bytes leverages simd well."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Table store (content, embedding, metadata, uploaded_at, id)>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = [dict(content=t, embedding=e.ravel().tobytes()) for t,e in zip(txts,embs[:-1])]\n",
    "store.insert_all(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### search"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "You can search through results using the `search` method of the database. the results are automatically reranked. Turn it ooff by passing `rrf=False`"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "> These results are not very meaningful since we're using a naive tfidf vectoriser. Check the examples folder for more meaningful examples with onnx based text encoders."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 2, 'content': \"I'm hungry\"},\n",
       " {'id': 1, 'content': 'this is a text'},\n",
       " {'id': 3, 'content': \"Let's play! shall we?\"}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "db.search(q, embs[-1].ravel().tobytes(), columns=['id', 'content'])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Turning off reranking can help you understand where the results are coming from."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fts': [],\n",
       " 'vec': [{'id': 2, 'content': \"I'm hungry\"},\n",
       "  {'id': 1, 'content': 'this is a text'},\n",
       "  {'id': 3, 'content': \"Let's play! shall we?\"}]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "db.search(q, embs[-1].ravel().tobytes(), columns=['id', 'content'], rrf=False)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "- Check out the `data` module for document and code parsing utilities.\n",
    "- Check out the `utils` module for onnx based text encoders.\n",
    "- Check out the `examples` folder for complete examples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
