{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceb2c60851cad223",
   "metadata": {},
   "source": [
    "# data\n",
    "\n",
    "> Building blocks for ingesting and querying data with thedu with some additional utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd2cae8a5079bb4",
   "metadata": {},
   "source": "We will build a simple ingestion pipeline to ingest pdf documents into thedu database for searching."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58f453150128627",
   "metadata": {},
   "outputs": [],
   "source": "#| default_exp data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1e955269e0d234",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from chonkie import Pipeline\n",
    "from fastcore.all import AttrDictDefault, L, dict2obj, concat, patch\n",
    "from fastlite import Database\n",
    "import os\n",
    "import pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dc1b52ef9593bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _read_page(page: pymupdf.Page # PyMuPDF Page object\n",
    "              ) -> AttrDictDefault:\n",
    "    '''Return a serialisable snapshot of all common page data.'''\n",
    "    tp = page.get_textpage()\n",
    "    def R(r): return tuple(r) if r else None\n",
    "    anns = [dict(xref=a.xref, type=a.type[1], rect=R(a.rect), info=a.info,\n",
    "                 colors=a.colors, border=getattr(a, 'border', None), uri=getattr(a, 'uri', None))\n",
    "            for a in page.annots()]\n",
    "    wids = [dict(xref=w.xref,field_name=w.field_name, field_type=w.field_type_string,\n",
    "                rect=R(w.rect), value=w.field_value)\n",
    "            for w in page.widgets()] if callable(getattr(page, 'widgets', None)) else []\n",
    "\n",
    "    return dict2obj(dict(number=page.number,\n",
    "        rect=R(getattr(page, 'rect', None)), mediabox=R(getattr(page, 'mediabox', getattr(page, 'rect', None))),\n",
    "        rotation=page.rotation, xref=getattr(page, 'xref', None), text_plain=page.get_text('text', textpage=tp),\n",
    "        text_rawdict=page.get_text('rawdict', textpage=tp),  # includes text blocks and image placeholders\n",
    "        text_json=page.get_text('json', textpage=tp),\n",
    "        # graphics and resources\n",
    "        links=page.get_links(), annotations=anns, widgets=wids,\n",
    "        images=page.get_images(full=True),      # list of image tuples\n",
    "        drawings=page.get_drawings()        # vector drawing ops\n",
    "    ))\n",
    "\n",
    "def read_pdf(pth: str|os.PathLike # path to PDF file\n",
    "            ) -> AttrDictDefault:\n",
    "    '''Read a PDF file and return a list of page data.'''\n",
    "    doc = pymupdf.open(pth)\n",
    "    return dict2obj(dict(name=doc.metadata['title'],num_pages=doc.page_count,metadata=doc.metadata,toc=doc.get_toc(),pages=L([_read_page(p) for p in doc])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d011bd4cd59886",
   "metadata": {},
   "source": "#### Some utilities for pdf processing "
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ddd4799f6b0d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def pymupdf2txt(doc): return '\\n\\n'.join([p.text_plain for p in doc.pages])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa2d46fdfc3b93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def pdf_pipe():\n",
    "    'Return the default chunking and embedding pipeline.'\n",
    "    return (\n",
    "            Pipeline()\n",
    "            .chunk_with('recursive', tokenizer='gpt2', chunk_size=2048)\n",
    "            .chunk_with('semantic', chunk_size=1024)\n",
    "            .refine_with('overlap', context_size=128)\n",
    "            .refine_with('embeddings', embedding_model='minishlab/potion-retrieval-32M')\n",
    "        )\n",
    "\n",
    "@patch\n",
    "def pdf_ingest(\n",
    "    self: Database,                # thedu database connection\n",
    "    pdf_doc: dict|os.PathLike,     # a pdf document or path. Use `read_pdf` to read from path\n",
    "    chunk_embed_pipe:Pipeline=None,# chunking and embedding pipeline. If None, use default chonkie pipeline\n",
    "    docs_tbl: str = 'docs',        # docs table name\n",
    "    content_tbl: str = 'content',  # content table name\n",
    "):\n",
    "    'Ingest PDF documents into thedu.'\n",
    "    if isinstance(pdf_doc, (str, os.PathLike)): pdf_doc = read_pdf(pdf_doc)\n",
    "    if isinstance(pdf_doc, dict): assert 'pages' in pdf_doc, 'Invalid PDF document dictionary. Use `read_pdf` to read from path.'\n",
    "    if not chunk_embed_pipe: chunk_embed_pipe = pdf_pipe()\n",
    "    self.store(chunk_embed_pipe.run(pymupdf2txt(pdf_doc)).chunks, name=pdf_doc.name, metadata=pdf_doc.metadata,\n",
    "               doc_tbl=docs_tbl, content_tbl=content_tbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f887a2d1e48c7e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def clean(q:str  # query to be passed for fts search\n",
    "          ):\n",
    "    '''Clean the query by removing * and returning None for empty queries.'''\n",
    "    return q.replace('*', '') if q.strip() else None\n",
    "\n",
    "def add_wc(q:str  # query to be passed for fts search\n",
    "           ):\n",
    "    '''Add wild card * to each word in the query.'''\n",
    "    return ' '.join(map(lambda w: w + '*', q.split(' ')))\n",
    "\n",
    "def mk_wider(q:str  # query to be passed for fts search\n",
    "             ):\n",
    "    '''Widen the query by joining words with OR operator.'''\n",
    "    return ' OR '.join(map(lambda w: f'{w}', q.split(' ')))\n",
    "\n",
    "def kw(q:str  # query to be passed for fts search\n",
    "       ):\n",
    "    '''Extract keywords from the query using YAKE library.'''\n",
    "    from yake import KeywordExtractor as KW\n",
    "    return ' '.join((set(concat([k.split(' ') for k, s in KW().extract_keywords(q)]))))\n",
    "\n",
    "def pre(q:str,          # query to be passed for fts search\n",
    "        wc=True,        # add wild card to each word\n",
    "        wide=True,      # widen the query with OR operator\n",
    "        extract_kw=True # extract keywords from the query\n",
    "        ):\n",
    "    '''Preprocess the query for fts search.'''\n",
    "    q = clean(q)\n",
    "    if not q.strip(): return ''\n",
    "    if extract_kw: q = kw(q)\n",
    "    if wc: q = add_wc(q)\n",
    "    if wide: q = mk_wider(q)\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf6852b35d08be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae77ab3a5f02bfa8",
   "metadata": {},
   "outputs": [],
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
