# utils


<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->

Let’s load some default models that work well off the box for various
tasks

FastEncode is an onnx based embedding model wrapper that can work with
most onnx model with a huggingface tokenizer. (The Qwen models are a bit
tricky due to their padding token handling so they need a custom wrapper
which we will add later)

    /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages/usearch/__init__.py:125: UserWarning: Will download `usearch_sqlite` binary from GitHub.
      warnings.warn("Will download `usearch_sqlite` binary from GitHub.", UserWarning)

------------------------------------------------------------------------

<a
href="https://github.com/Karthik777/litesearch/blob/main/litesearch/utils.py#L101"
target="_blank" style="float:right; font-size:smaller">source</a>

### download_model

>  download_model (repo_id='onnx-community/embeddinggemma-300m-ONNX',
>                      md='onnx-community/embeddinggemma-300m-ONNX', token=None)

*Download model from HF hub*

<table>
<colgroup>
<col style="width: 6%" />
<col style="width: 25%" />
<col style="width: 34%" />
<col style="width: 34%" />
</colgroup>
<thead>
<tr>
<th></th>
<th><strong>Type</strong></th>
<th><strong>Default</strong></th>
<th><strong>Details</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>repo_id</td>
<td>str</td>
<td>onnx-community/embeddinggemma-300m-ONNX</td>
<td>model repo on HF</td>
</tr>
<tr>
<td>md</td>
<td>str</td>
<td>onnx-community/embeddinggemma-300m-ONNX</td>
<td>local model dir</td>
</tr>
<tr>
<td>token</td>
<td>NoneType</td>
<td>None</td>
<td>HF token. you can also set HF_TOKEN env variable</td>
</tr>
</tbody>
</table>

------------------------------------------------------------------------

<a
href="https://github.com/Karthik777/litesearch/blob/main/litesearch/utils.py#L27"
target="_blank" style="float:right; font-size:smaller">source</a>

### FastEncode

>  FastEncode (model_dict={'model': 'onnx-
>                  community/embeddinggemma-300m-ONNX', 'onnx_path':
>                  'onnx/model.onnx', 'prompt': {'document': 'Instruct: document
>                  \n document: {text}', 'query': 'Instruct: query \n query:
>                  {text}'}}, repo_id=None, md=None, md_nm=None, normalize=True,
>                  dtype=<class 'numpy.float16'>, tti=False, prompt=None,
>                  hf_token=None)

*Fast ONNX-based text encoder*

<table>
<colgroup>
<col style="width: 6%" />
<col style="width: 25%" />
<col style="width: 34%" />
<col style="width: 34%" />
</colgroup>
<thead>
<tr>
<th></th>
<th><strong>Type</strong></th>
<th><strong>Default</strong></th>
<th><strong>Details</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>model_dict</td>
<td>AttrDict</td>
<td>{‘model’: ‘onnx-community/embeddinggemma-300m-ONNX’, ‘onnx_path’:
‘onnx/model.onnx’, ‘prompt’: {‘document’: ‘Instruct: document document:
{text}’, ‘query’: ‘Instruct: query query: {text}’}}</td>
<td>model dict with model repo, onnx path and prompt templates</td>
</tr>
<tr>
<td>repo_id</td>
<td>NoneType</td>
<td>None</td>
<td>model repo on HF. needs to have onnx model file</td>
</tr>
<tr>
<td>md</td>
<td>NoneType</td>
<td>None</td>
<td>local model dir</td>
</tr>
<tr>
<td>md_nm</td>
<td>NoneType</td>
<td>None</td>
<td>onnx model file name</td>
</tr>
<tr>
<td>normalize</td>
<td>bool</td>
<td>True</td>
<td>normalize embeddings</td>
</tr>
<tr>
<td>dtype</td>
<td>type</td>
<td>float16</td>
<td>output dtype</td>
</tr>
<tr>
<td>tti</td>
<td>bool</td>
<td>False</td>
<td>use token type ids</td>
</tr>
<tr>
<td>prompt</td>
<td>NoneType</td>
<td>None</td>
<td>prompt templates</td>
</tr>
<tr>
<td>hf_token</td>
<td>NoneType</td>
<td>None</td>
<td>HF token. you can also set HF_TOKEN env variable</td>
</tr>
</tbody>
</table>

Let’s quickly check if the encoder is working

``` python
enc=FastEncode()
```

    2025-12-16 22:58:02.611162903 [W:onnxruntime:, transformer_memcpy.cc:111 ApplyImpl] 736 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.

``` python
enc.encode_document(['This is a test', 'Another test'])
```

    array([[ 0.05774 ,  0.001704,  0.002562, ..., -0.06177 , -0.00661 ,
             0.03174 ],
           [ 0.02939 , -0.008194, -0.00918 , ..., -0.02846 , -0.002222,
             0.02847 ]], shape=(2, 768), dtype=float16)

``` python
modern_enc=FastEncode(modernbert)
```

``` python
modern_enc.encode_query(['This is a test', 'Another test'])
```

    array([[-0.05026 , -0.04352 , -0.0171  , ..., -0.04974 ,  0.01598 ,
            -0.07056 ],
           [-0.05093 , -0.02133 , -0.0368  , ..., -0.10736 , -0.000944,
            -0.01177 ]], shape=(2, 768), dtype=float16)
