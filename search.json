[
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "Introduction\nWe often have to go through a whole bunch of hoops to get documents processed and ready for searching through them. litesearch plans to make this as easy as possible by providing simple building blocks to set up a database with FTS5 and vector search capabilities.\n/opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages/usearch/__init__.py:125: UserWarning: Will download `usearch_sqlite` binary from GitHub.\n  warnings.warn(\"Will download `usearch_sqlite` binary from GitHub.\", UserWarning)\n\nsource\n\n\nDatabase.query\n\n Database.query (sql:str, params:Union[Iterable,dict,NoneType]=None)\n\nExecute a query and return results as a list of AttrDict\n\nSimple Docs table setup\n\n\nsource\n\n\nDatabase.get_store\n\n Database.get_store (name:str='store', hash:bool=False, **kw)\n\nMake a sql table for content storage with FTS5 and vector search capabilities\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nname\nstr\nstore\ntable name\n\n\nhash\nbool\nFalse\nwhether to create hash index on content\n\n\nkw\nVAR_KEYWORD\n\n\n\n\n\n\nsource\n\n\ndatabase\n\n database (pth_or_uri:str=':memory:', wal:bool=True, sem_search:bool=True,\n           **kw)\n\nSet up a database connection and load usearch extensions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npth_or_uri\nstr\n:memory:\nthe database name or URL\n\n\nwal\nbool\nTrue\nuse WAL mode\n\n\nsem_search\nbool\nTrue\nenable usearch extensions\n\n\nkw\nVAR_KEYWORD\n\n\n\n\nReturns\nDatabase\n\nadditional args to pass to apswutils database\n\n\n\n\nsource\n\n\nDatabase.search\n\n Database.search (q:str, emb:bytes, columns:list=None, where:str=None,\n                  where_args:dict=None, limit:int|None=50,\n                  offset:int|None=None, table_name='store',\n                  emb_col='embedding', emb_metric:str='cosine', rrf=True,\n                  dtype=&lt;class 'numpy.float16'&gt;)\n\nSearch the litesearch store with fts and vector search combined.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nq\nstr\n\nquery string\n\n\nemb\nbytes\n\nembedding vector\n\n\ncolumns\nlist\nNone\ncolumns to return\n\n\nwhere\nstr\nNone\nadditional where clause\n\n\nwhere_args\ndict\nNone\nargs for where clause\n\n\nlimit\nint | None\n50\nlimit on number of results\n\n\noffset\nint | None\nNone\noffset for results\n\n\ntable_name\nstr\nstore\ntable name\n\n\nemb_col\nstr\nembedding\nembedding column name\n\n\nemb_metric\nstr\ncosine\nembedding distance metric (cosine,sqeuclidean,inner,divergence)\n\n\nrrf\nbool\nTrue\nneed to rerank results with reciprocal rank fusion\n\n\ndtype\ntype\nfloat16\nembedding dtype\n\n\n\nLet’s test it out. We will create a database, run embedding comparisons, create a store and run search\n\ndb = database()\n\nThe fastlite database is set up with usearch extensions. Let’s run some distance calculations.\n\nembs = dict(\n    v1=np.ones((100,),dtype=np.float32).tobytes(),      # vector of ones\n    v2=np.zeros((100,),dtype=np.float32).tobytes(),     # vector of zeros\n    v3=np.full((100,),0.25,dtype=np.float32).tobytes()  # vector of 0.25s\n)\ndef dist_q(metric):\n    return db.q(f'''\n        select\n            distance_{metric}_f32(:v1,:v2) as {metric}_v1_v2,\n            distance_{metric}_f32(:v1,:v3) as {metric}_v1_v3,\n            distance_{metric}_f32(:v2,:v3) as {metric}_v2_v3\n    ''', embs)\n\nfor fn in ['sqeuclidean', 'divergence', 'inner', 'cosine']: print(dist_q(fn))\n\n[{'sqeuclidean_v1_v2': 100.0, 'sqeuclidean_v1_v3': 56.25, 'sqeuclidean_v2_v3': 6.25}]\n[{'divergence_v1_v2': 34.657352447509766, 'divergence_v1_v3': 12.046551704406738, 'divergence_v2_v3': 8.66433334350586}]\n[{'inner_v1_v2': 1.0, 'inner_v1_v3': -24.0, 'inner_v2_v3': 1.0}]\n[{'cosine_v1_v2': 1.0, 'cosine_v1_v3': 0.0, 'cosine_v2_v3': 1.0}]\n\n\n\ndb.get_store()\nif 'store' in db.t: print('store is created')\nprint('detected fts table: ',db.t.store.detect_fts())\nprint('Search results:', len(db.search('h',np.zeros((100,)).tobytes()))) # there is no data yet, so should be 0\n\nstore is created\ndetected fts table:  store_fts\nSearch results: 0\n\n\nWe can also create a store with hash index on content. Useful for code search applications\n\nst=db.get_store(name='my_store', hash=True)\nst.insert_all([dict(content='hello world', embedding=np.ones((100,),dtype=np.float16).tobytes()),\n                           dict(content='hi there', embedding=np.full((100,),0.5,dtype=np.float16).tobytes()),\n                           dict(content='goodbye now', embedding=np.zeros((100,),dtype=np.float16).tobytes())],upsert=True,hash_id='id')\nst(select='id,content')\n\n[{'id': '250ce2bffa97ab21fa9ab2922d19993454a0cf28', 'content': 'hello world'},\n {'id': 'c89f43361891bfab9290bcebf182fa5978f89700', 'content': 'hi there'},\n {'id': '882293d5e5c3d3e04e8e0c4f7c01efba904d0932', 'content': 'goodbye now'}]\n\n\nLet’s run a search again.\n\ndb.search(q='hello', emb=np.full((100,),0.25, dtype=np.float16).tobytes(), columns=['content'], table_name='my_store',limit=2, rrf=False)\n\n{'fts': [{'content': 'hello world'}],\n 'vec': [{'content': 'hello world'}, {'content': 'hi there'}]}\n\n\nNow, let’s try the same but with a broader query.\n\ndb.search(q='goodbye OR hi', emb=np.full((100,),0,dtype=np.float16).tobytes(), columns=['content'], table_name='my_store',limit=2)\n\n[{'content': 'goodbye now'}, {'content': 'hello world'}]\n\n\nYou can use different kind of embedding metrics as well. The default is cosine. Let’s try with divergence distance\n\ndb.search(q='goodbye OR hi', emb=np.full((100,),0,dtype=np.float16).tobytes(), columns=['content'], table_name='my_store',limit=2, emb_metric='divergence')\n\n[{'content': 'goodbye now'}, {'content': 'hi there'}]",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "litesearch",
    "section": "",
    "text": "NB If you’re reading this in GitHub readme, I recommend you read the more nicely formatted documentation format of this tutorial.\nLitesearch is a lightweight library to set up a fastlite database with FTS5 and vector search capabilities using usearch.\nLitesearch uses usearch sqlite extensions to provide fast vector search capabilities and combines it with sqlite’s FTS5 capabilities to provide hybrid search. - Litesearch uses fastlite, which is a lightweight wrapper around SQLite that makes SQLite database management delightful. It uses apsw rather than sqlite3 and provides best practices OOB. - Usearch is a cross-language package which provides vector search capabilities. We’re using its sqlite extensions here to provide fast vector search capabilities.\nLite search provides a simple way to setup this database using the database() method. You get a store with FTS5 and vector search capabilities using the get_store() method and you can search through the contents using the search() method.\nLitesearch also provides document and code manipulation tools as part of the data module and onnx based text encoders as part of the utils module. - litesearch extends pymupdf Document and Page classes to extract texts, images and links easily. - litesearch provides onnx based text encoders which can be used to generate embeddings for documents and queries. - litesearch provides a quick code parsing utility to parse python files into code chunks for ingestion.",
    "crumbs": [
      "litesearch"
    ]
  },
  {
    "objectID": "index.html#get-started",
    "href": "index.html#get-started",
    "title": "litesearch",
    "section": "Get Started",
    "text": "Get Started\nfastlite and usearch will be installed automatically with litesearch if you do not have it already.\n\n!pip install litesearch -qq\n\nLitesearch only adds dependencies it needs, so you can use import * from litesearch without worrying about heavy dependencies. &gt; First time import will try to setup usearch extensions and installing libsqlite3 if you do not have it already. mac also needs an extra step to add libsqlite3 into it’s LC_PATH. Check postfix.py for details.\n\nfrom litesearch import *\n\n\ndatabase\n\ndb = database()\ndb.q('select sqlite_version() as sqlite_version')\n\n[{'sqlite_version': '3.51.1'}]\n\n\nLet’s try some of usearch’s distance functions\n\nimport numpy as np\n\n\nembs = dict(\n    v1=np.ones((100,),dtype=np.float32).tobytes(),      # vector of ones\n    v2=np.zeros((100,),dtype=np.float32).tobytes(),     # vector of zeros\n    v3=np.full((100,),0.25,dtype=np.float32).tobytes()  # vector of 0.25s\n)\ndef dist_q(metric):\n    return db.q(f'''\n        select\n            distance_{metric}_f32(:v1,:v2) as {metric}_v1_v2,\n            distance_{metric}_f32(:v1,:v3) as {metric}_v1_v3,\n            distance_{metric}_f32(:v2,:v3) as {metric}_v2_v3\n    ''', embs)\n\nfor fn in ['sqeuclidean', 'divergence', 'inner', 'cosine']: print(dist_q(fn))\n\n[{'sqeuclidean_v1_v2': 100.0, 'sqeuclidean_v1_v3': 56.25, 'sqeuclidean_v2_v3': 6.25}]\n[{'divergence_v1_v2': 34.657352447509766, 'divergence_v1_v3': 12.046551704406738, 'divergence_v2_v3': 8.66433334350586}]\n[{'inner_v1_v2': 1.0, 'inner_v1_v3': -24.0, 'inner_v2_v3': 1.0}]\n[{'cosine_v1_v2': 1.0, 'cosine_v1_v3': 0.0, 'cosine_v2_v3': 1.0}]\n\n\n\n\nstore\n\nA store is a table with FTS5 and vector search capabilities.\n\n\nstore = db.get_store()\nstore.schema\n\n'CREATE TABLE [store] (\\n   [content] TEXT NOT NULL,\\n   [embedding] BLOB,\\n   [metadata] TEXT,\\n   [uploaded_at] FLOAT DEFAULT CURRENT_TIMESTAMP,\\n   [id] INTEGER PRIMARY KEY\\n)'\n\n\nLet’s use a naive embedder for testing. &gt; Checkout FastEncode in utils module for onnx based text encoders. &gt; Check the examples folder for usage.\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n\ntxts, q = ['this is a text', \"I'm hungry\", \"Let's play! shall we?\"], 'playing hungry'\n# this is naive vectoriser intended to showcase litesearch. In practice, use a proper text encoder.\ndef embed_texts(texts): return TfidfVectorizer(max_features=20000, stop_words='english').fit_transform(texts).toarray().astype(np.float16)\nembs = embed_texts(txts + [q])  # last one is query\nembs\n\narray([[0.   , 0.   , 0.   , 0.   , 0.   , 1.   ],\n       [1.   , 0.   , 0.   , 0.   , 0.   , 0.   ],\n       [0.   , 0.577, 0.577, 0.   , 0.577, 0.   ],\n       [0.619, 0.   , 0.   , 0.785, 0.   , 0.   ]], dtype=float16)\n\n\n\nusearch also works with json embeddings, but using bytes leverages simd well.\n\n\nrows = [dict(content=t, embedding=e.ravel().tobytes()) for t,e in zip(txts,embs[:-1])]\nstore.insert_all(rows)\n\n&lt;Table store (content, embedding, metadata, uploaded_at, id)&gt;\n\n\n\nsearch\nYou can search through results using the search method of the database. the results are automatically reranked. Turn it ooff by passing rrf=False\n\nThese results are not very meaningful since we’re using a naive tfidf vectoriser. Check the examples folder for more meaningful examples with onnx based text encoders.\n\n\ndb.search(q, embs[-1].ravel().tobytes(), columns=['id', 'content'])\n\n[{'id': 2, 'content': \"I'm hungry\"},\n {'id': 1, 'content': 'this is a text'},\n {'id': 3, 'content': \"Let's play! shall we?\"}]\n\n\nTurning off reranking can help you understand where the results are coming from.\n\ndb.search(q, embs[-1].ravel().tobytes(), columns=['id', 'content'], rrf=False)\n\n{'fts': [],\n 'vec': [{'id': 2, 'content': \"I'm hungry\"},\n  {'id': 1, 'content': 'this is a text'},\n  {'id': 3, 'content': \"Let's play! shall we?\"}]}",
    "crumbs": [
      "litesearch"
    ]
  },
  {
    "objectID": "index.html#next-steps",
    "href": "index.html#next-steps",
    "title": "litesearch",
    "section": "Next steps",
    "text": "Next steps\n\nCheck out the data module for document and code parsing utilities.\nCheck out the utils module for onnx based text encoders.\nCheck out the examples folder for complete examples.",
    "crumbs": [
      "litesearch"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "data",
    "section": "",
    "text": "We will build a simple ingestion pipeline to ingest pdf documents into litesearch database for searching.\nExtensions to pymupdf Document and Page classes to extract texts, images and links\n/opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages/usearch/__init__.py:125: UserWarning: Will download `usearch_sqlite` binary from GitHub.\n  warnings.warn(\"Will download `usearch_sqlite` binary from GitHub.\", UserWarning)\n\nsource\n\nDocument.ext_imgs\n\n Document.ext_imgs (st=0, end=-1)\n\n\nsource\n\n\nDocument.ext_im\n\n Document.ext_im (it=None)\n\n\nsource\n\n\nDocument.get_links\n\n Document.get_links (st=0, end=-1)\n\n\nsource\n\n\nDocument.get_texts\n\n Document.get_texts (st=0, end=-1, **kw)\n\nCode extraction utilities\n\nsource\n\n\npyparse\n\n pyparse (p:pathlib.Path=None, code:str=None, imports=False)\n\nParse a code string or python file and return code chunks as list of dicts with content and metadata.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\np\nPath\nNone\npath to a python file\n\n\ncode\nstr\nNone\ncode string to parse\n\n\nimports\nbool\nFalse\ninclude import statements as code chunks\n\n\nReturns\nL\n\n\n\n\n\nYou can use pyparse to extract code chunks from a python file or code string.\n\ntxt = \"\"\"\nfrom fastcore.all import *\na=1\nclass SomeClass:\n    def __init__(self,x): store_attr()\n    def method(self): return self.x + a\n \"\"\"\npyparse(code=txt)\n\n(#2) [{'content': 'a=1', 'metadata': {'path': None, 'uploaded_at': None, 'name': None, 'type': 'Assign', 'lineno': 3, 'end_lineno': 3}},{'content': 'class SomeClass:\\n    def __init__(self,x): store_attr()\\n    def method(self): return self.x + a', 'metadata': {'path': None, 'uploaded_at': None, 'name': 'SomeClass', 'type': 'ClassDef', 'lineno': 4, 'end_lineno': 6}}]\n\n\nSetting imports to True will also include import statements as code chunks.\n\npyparse(code=txt, imports=True)\n\n(#3) [{'content': 'from fastcore.all import *', 'metadata': {'path': None, 'uploaded_at': None, 'name': None, 'type': 'ImportFrom', 'lineno': 2, 'end_lineno': 2}},{'content': 'a=1', 'metadata': {'path': None, 'uploaded_at': None, 'name': None, 'type': 'Assign', 'lineno': 3, 'end_lineno': 3}},{'content': 'class SomeClass:\\n    def __init__(self,x): store_attr()\\n    def method(self): return self.x + a', 'metadata': {'path': None, 'uploaded_at': None, 'name': 'SomeClass', 'type': 'ClassDef', 'lineno': 4, 'end_lineno': 6}}]\n\n\n\nsource\n\n\npkg2chunks\n\n pkg2chunks (pkg:str, imports:bool=False, **kw)\n\nReturn code chunks from a package with extra metadata.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npkg\nstr\n\npackage name\n\n\nimports\nbool\nFalse\ninclude import statements as code chunks\n\n\nkw\nVAR_KEYWORD\n\n\n\n\nReturns\nL\n\nadditional args to pass to pkg2files\n\n\n\n\nsource\n\n\npkg2files\n\n pkg2files (pkg:str, file_glob:str='*.py', skip_file_glob:str='_*', skip_f\n            ile_re='(^__init__\\\\.py$|^setup\\\\.py$|^conftest\\\\.py$|^test_.*\n            \\\\.py$|^tests?\\\\.py$|^.*_test\\\\.py$)', skip_folder_re='(^tests\n            ?$|^__pycache__$|^\\\\.eggs$|^\\\\.mypy_cache$|^\\\\.tox$|^examples?\n            $|^docs?$|^build$|^dist$|^\\\\.git$|^\\\\.ipynb_checkpoints$)',\n            recursive:bool=True, symlinks:bool=True, file_re:str=None,\n            folder_re:str=None, func:callable=&lt;function join&gt;,\n            ret_folders:bool=False, sort:bool=True)\n\nReturn list of python files in a package excluding tests and setup files.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npkg\nstr\n\npackage name\n\n\nfile_glob\nstr\n*.py\nfile glob to match\n\n\nskip_file_glob\nstr\n_*\nfile glob to skip\n\n\nskip_file_re\nstr\n(^init.py\\(\\|\\^setup\\.py\\)|^conftest.py\\(\\|\\^test_.*\\.py\\)|^tests?.py\\(\\|\\^.*_test\\.py\\))\nregex to skip files\n\n\nskip_folder_re\nstr\n(^tests?\\(\\|\\^__pycache__\\)|^.eggs\\(\\|\\^\\.mypy_cache\\)|^.tox\\(\\|\\^examples?\\)|^docs?\\(\\|\\^build\\)|^dist\\(\\|\\^\\.git\\)|^.ipynb_checkpoints$)\nregex to skip folders\n\n\nrecursive\nbool\nTrue\nsearch subfolders\n\n\nsymlinks\nbool\nTrue\nfollow symlinks?\n\n\nfile_re\nstr\nNone\nOnly include files matching regex\n\n\nfolder_re\nstr\nNone\nOnly enter folders matching regex\n\n\nfunc\ncallable\njoin\nfunction to apply to each matched file\n\n\nret_folders\nbool\nFalse\nreturn folders, not just files\n\n\nsort\nbool\nTrue\nsort files by name within each folder\n\n\nReturns\nL\n\nadditional args to pass to globtastic\n\n\n\npkg2chunks can be used to extract code chunks from an entire package installed in your environment.\n\nchunks=pkg2chunks('fastlite')\nchunks.filter(lambda d: d['metadata']['type']=='FunctionDef')[0]\n\n{'content': 'def t(self:Database): return _TablesGetter(self)',\n 'metadata': {'path': '/Users/71293/code/litesearch/.venv/lib/python3.13/site-packages/fastlite/core.py',\n  'uploaded_at': 1752468812.9739048,\n  'name': 't',\n  'type': 'FunctionDef',\n  'lineno': 44,\n  'end_lineno': 44,\n  'package': 'fastlite',\n  'version': '0.2.1'}}\n\n\n\nsource\n\n\ninstalled_packages\n\n installed_packages (nms:list=None)\n\nReturn list of installed packages. If nms is provided, return only those packages.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnms\nlist\nNone\nlist of package names\n\n\nReturns\nL\n\n\n\n\n\nGet list of installed packages in your environment using installed_packages. If you pass a list of package names, it only returns them if they exist in your environment.\n\ninstalled_packages(['fstlite']) # non existent package\ninstalled_packages(['fastlite']) # existing package\ninstalled_packages() # all installed packages that are not stdlib\n\n(#179) ['litesearch','shellingham','jiter','ipykernel','simsimd','threadpoolctl','coloredlogs','uri-template','humanfriendly','socksio','rfc3339-validator','pexpect','jupyterlab-quarto','fqdn','requests','babel','rich','traitlets','tokenizers','urllib3'...]\n\n\nQuery Preprocessing utilities\n\nsource\n\n\npre\n\n pre (q:str, wc=True, wide=True, extract_kw=True)\n\nPreprocess the query for fts search.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nq\nstr\n\nquery to be passed for fts search\n\n\nwc\nbool\nTrue\nadd wild card to each word\n\n\nwide\nbool\nTrue\nwiden the query with OR operator\n\n\nextract_kw\nbool\nTrue\nextract keywords from the query\n\n\n\n\nsource\n\n\nkw\n\n kw (q:str)\n\nExtract keywords from the query using YAKE library.\n\n\n\n\nType\nDetails\n\n\n\n\nq\nstr\nquery to be passed for fts search\n\n\n\n\nsource\n\n\nmk_wider\n\n mk_wider (q:str)\n\nWiden the query by joining words with OR operator.\n\n\n\n\nType\nDetails\n\n\n\n\nq\nstr\nquery to be passed for fts search\n\n\n\n\nsource\n\n\nadd_wc\n\n add_wc (q:str)\n\nAdd wild card  to each word in the query.*\n\n\n\n\nType\nDetails\n\n\n\n\nq\nstr\nquery to be passed for fts search\n\n\n\n\nsource\n\n\nclean\n\n clean (q:str)\n\nClean the query by removing  and returning None for empty queries.*\n\n\n\n\nType\nDetails\n\n\n\n\nq\nstr\nquery to be passed for fts search\n\n\n\nYou can clean queries passed into fts search using clean, add wild cards using add_wc, widen the query using mk_wider and extract keywords using kw. You can combine all these using pre function.\n\nq = 'This is a sample query'\nprint('preprocessed q with defaults: `%s`' %pre(q))\nprint('keywords extracted: `%s`' %pre(q, wc=False, wide=False))\nprint('q with wild card: `%s`' %pre(q, extract_kw=False, wide=False, wc=True))\n\npreprocessed q with defaults: `query* OR sample*`\nkeywords extracted: `query sample`\nq with wild card: `This* is* a* sample* query*`",
    "crumbs": [
      "data"
    ]
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "utils",
    "section": "",
    "text": "Let’s load some default models that work well off the box for various tasks\nFastEncode is an onnx based embedding model wrapper that can work with most onnx model with a huggingface tokenizer. (The Qwen models are a bit tricky due to their padding token handling so they need a custom wrapper which we will add later)\n/opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages/usearch/__init__.py:125: UserWarning: Will download `usearch_sqlite` binary from GitHub.\n  warnings.warn(\"Will download `usearch_sqlite` binary from GitHub.\", UserWarning)\n\nsource\n\ndownload_model\n\n download_model (repo_id='onnx-community/embeddinggemma-300m-ONNX',\n                 md='onnx-community/embeddinggemma-300m-ONNX', token=None)\n\nDownload model from HF hub\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nrepo_id\nstr\nonnx-community/embeddinggemma-300m-ONNX\nmodel repo on HF\n\n\nmd\nstr\nonnx-community/embeddinggemma-300m-ONNX\nlocal model dir\n\n\ntoken\nNoneType\nNone\nHF token. you can also set HF_TOKEN env variable\n\n\n\n\nsource\n\n\nFastEncode\n\n FastEncode (model_dict={'model': 'onnx-\n             community/embeddinggemma-300m-ONNX', 'onnx_path':\n             'onnx/model.onnx', 'prompt': {'document': 'Instruct: document\n             \\n document: {text}', 'query': 'Instruct: query \\n query:\n             {text}'}}, repo_id=None, md=None, md_nm=None, normalize=True,\n             dtype=&lt;class 'numpy.float16'&gt;, tti=False, prompt=None,\n             hf_token=None)\n\nFast ONNX-based text encoder\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodel_dict\nAttrDict\n{‘model’: ‘onnx-community/embeddinggemma-300m-ONNX’, ‘onnx_path’: ‘onnx/model.onnx’, ‘prompt’: {‘document’: ‘Instruct: document document: {text}’, ‘query’: ‘Instruct: query query: {text}’}}\nmodel dict with model repo, onnx path and prompt templates\n\n\nrepo_id\nNoneType\nNone\nmodel repo on HF. needs to have onnx model file\n\n\nmd\nNoneType\nNone\nlocal model dir\n\n\nmd_nm\nNoneType\nNone\nonnx model file name\n\n\nnormalize\nbool\nTrue\nnormalize embeddings\n\n\ndtype\ntype\nfloat16\noutput dtype\n\n\ntti\nbool\nFalse\nuse token type ids\n\n\nprompt\nNoneType\nNone\nprompt templates\n\n\nhf_token\nNoneType\nNone\nHF token. you can also set HF_TOKEN env variable\n\n\n\nLet’s quickly check if the encoder is working\n\nenc=FastEncode()\n\n\n2025-12-16 22:58:02.611162903 [W:onnxruntime:, transformer_memcpy.cc:111 ApplyImpl] 736 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\n\n\n\n\n\nenc.encode_document(['This is a test', 'Another test'])\n\narray([[ 0.05774 ,  0.001704,  0.002562, ..., -0.06177 , -0.00661 ,\n         0.03174 ],\n       [ 0.02939 , -0.008194, -0.00918 , ..., -0.02846 , -0.002222,\n         0.02847 ]], shape=(2, 768), dtype=float16)\n\n\n\nmodern_enc=FastEncode(modernbert)\n\n\nmodern_enc.encode_query(['This is a test', 'Another test'])\n\narray([[-0.05026 , -0.04352 , -0.0171  , ..., -0.04974 ,  0.01598 ,\n        -0.07056 ],\n       [-0.05093 , -0.02133 , -0.0368  , ..., -0.10736 , -0.000944,\n        -0.01177 ]], shape=(2, 768), dtype=float16)",
    "crumbs": [
      "utils"
    ]
  },
  {
    "objectID": "postfix.html",
    "href": "postfix.html",
    "title": "postfix",
    "section": "",
    "text": "Post Import Fixes\n\nPost fixes that need to be applied after importing litesearch package.\n\nWe need to apply some post fix for mac to load the dylib properly for usearch sqlite path to be loaded correctly.\n/opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages/usearch/__init__.py:125: UserWarning: Will download `usearch_sqlite` binary from GitHub.\n  warnings.warn(\"Will download `usearch_sqlite` binary from GitHub.\", UserWarning)\n\nsource\n\n\nusearch_fix\n\n usearch_fix (v:bool=False)\n\nApply usearch macOS fix if on macOS.",
    "crumbs": [
      "postfix"
    ]
  }
]